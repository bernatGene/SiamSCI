{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese network implementation with the strips dataset\n",
    "\n",
    "With denoised and averaged images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structure\n",
    "\n",
    "The images themselves are stored already cut as strips, with the characteristic extracted by denoising them and getting the residual, as described in Lukas et al. We have an indexing file to retrieve them (info data) in the following structure:\n",
    "\n",
    "* Info data:\n",
    "    * Student 0:\n",
    "        * Image 0:\n",
    "            * Strip 0,0\n",
    "            * Strip 0,256\n",
    "            * Strip 0,512\n",
    "            * ...\n",
    "        * Image 1:\n",
    "            * Strip 0,0\n",
    "            * ...\n",
    "        * ...\n",
    "    * Student 1:\n",
    "        * ...\n",
    "\n",
    "Strip naming convention:\n",
    "\n",
    "* Eurecom_NNN_picXG_III_XXXX_YYYY.PNG -> Strip from:\n",
    "    * position (XXXX,YYYY) (top, left), from: \n",
    "    * image picXG_III, from:\n",
    "    * Student with id NNN\n",
    "\n",
    "Then, the function get_batch generates a random sample from the dataset. It allows to draw either from the training or validation split of the dataset (defined at the level of the students, 0.8 of the students are in train and the rest in validation).\n",
    "The generated batch has the following shape:\n",
    "\n",
    "* (label, (batch_size, examples, height, width, channels), (batch_size, examples, height, width, channels))  Since images are grayscale, in fact channels should be 1\n",
    "\n",
    "More clearly:\n",
    "\n",
    "* batch:\n",
    "    * label 0:\n",
    "        * strip_x-y from Image j from Student i  <---------->  strip_w-t from Image k from Student l\n",
    "        * strip_x-y from Image j2 from Student i  <---------->  strip_w-t from Image k2 from Student l\n",
    "        * strip_x-y from Image j3 from Student i  <---------->  strip_w-t from Image k3 from Student l\n",
    "        * strip_x-y from Image j4 from Student i  <---------->  strip_w-t from Image k4 from Student l\n",
    "    * label 0:\n",
    "        * ...\n",
    "        \n",
    "    * label 1:\n",
    "        * strip_x-y from Image j from Student i  <---------->  strip_w-t from Image k from Student i\n",
    "        * strip_x-y from Image j2 from Student i  <---------->  strip_w-t from Image k2 from Student i\n",
    "        * strip_x-y from Image j3 from Student i  <---------->  strip_w-t from Image k3 from Student i\n",
    "        * strip_x-y from Image j4 from Student i  <---------->  strip_w-t from Image k4 from Student i\n",
    "    * label 1:\n",
    "        * ...\n",
    "        \n",
    "Note that ${i \\neq l, j \\neq j_i}$ etc. \n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pipeline\n",
    "\n",
    "During training, a batch is generated at random from the training slice, ensuring the previously described invariant. The next steps are as follows:\n",
    "\n",
    "* Images are normalized\n",
    "* We define the left and right input, according to the tuple in the pair (for label == 0, all strips belong to the same camera and position, while for label == 1, each input belongs to a different camera.)\n",
    "* All images from each input are averaged element-wise (separetely, of course)\n",
    "* The result is fed, one at a time to a CNN model defined at sequential_block. Note that the weights are the same for each input\n",
    "* The CNN outputs a feature vector of size 2048\n",
    "* The output from each of the inputs is compared by calculating the absolute distance between the two vectors\n",
    "* This result is fed into a final Dense layer, for classification, with sigmoid activation. \n",
    "    * A result close to 0 indicates that the strips from each input belong to different devices.\n",
    "    * A result close to 1 indicates that the strips from each input where taken by the same device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random as rng\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data = []\n",
    "with open('/home/data/strips_socrates/dataset_info.json') as json_file: \n",
    "    info_data = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIP_SIZE = 256\n",
    "SUB_STRIP_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(info_data, batch_size, examples, with_id=False, dataset = 'train'):\n",
    "    pairs = [np.zeros((batch_size, examples, STRIP_SIZE, STRIP_SIZE)) for i in range(2)]\n",
    "    labels = np.zeros((batch_size, ))\n",
    "    labels[batch_size//2:] = 1\n",
    "    split_index = int((len(info_data)-1) * 0.8)\n",
    "    if dataset == 'train':\n",
    "        students = [rng.randint(0, split_index) for _ in range(batch_size)] \n",
    "    elif dataset == 'whole':\n",
    "        students = [rng.randint(0, (len(info_data)-1)) for _ in range(batch_size)] \n",
    "    else:\n",
    "        students = [rng.randint(split_index+1, len(info_data)-1) for _ in range(batch_size)] \n",
    "    strips_loc = [rng.randint(0, len(info_data[i][0][1])-1) for i in students]\n",
    "    id_pairs = []\n",
    "    for i in range(batch_size):\n",
    "        std = students[i]\n",
    "        avl_images = [j for j in range(len(info_data[std]))]\n",
    "        exl = rng.sample(avl_images, examples)\n",
    "        avl_images = [j for j in avl_images if j not in exl]\n",
    "        srl = strips_loc[i]\n",
    "        strips1 = [ info_data[std][e][1][srl] for e in exl ]\n",
    "        strips2 = []\n",
    "        if i >= batch_size // 2:\n",
    "            exl2 = rng.sample(avl_images, examples)\n",
    "            strips2 = [info_data[std][e][1][srl] for e in exl2]\n",
    "        else:\n",
    "            std2 = (std + rng.randint(1, len(info_data)-1)) % len(info_data)\n",
    "            srl2 = rng.randint(0, len(info_data[std2][0][1])-1)\n",
    "            avl_images2 = [j for j in range(len(info_data[std2]))]\n",
    "            exl2 = rng.sample(avl_images2, examples)\n",
    "            strips2 = [ info_data[std2][e][1][srl2] for e in exl2]\n",
    "        for k in range(examples):\n",
    "            pairs[0][i,k,:,:] = cv2.imread(strips1[k], cv2.IMREAD_UNCHANGED)/255\n",
    "            pairs[1][i,k,:,:] = cv2.imread(strips2[k], cv2.IMREAD_UNCHANGED)/255\n",
    "        id_pairs.append((strips1, strips2))\n",
    "    if with_id:\n",
    "        return pairs, labels, id_pairs\n",
    "    else:\n",
    "        return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,l,ids = get_batch(info_data, 10, 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "/home/data/strips_socrates/108/Eurecom_108_picBG_019_256_1792.PNG\n",
      "/home/data/strips_socrates/108/Eurecom_108_picBG_034_256_1792.PNG\n",
      "/home/data/strips_socrates/108/Eurecom_108_picBG_018_256_1792.PNG\n",
      "/home/data/strips_socrates/108/Eurecom_108_picFG_040_256_1792.PNG\n",
      "/home/data/strips_socrates/130/Eurecom_130_picFG_004_2560_1792.PNG\n",
      "/home/data/strips_socrates/130/Eurecom_130_picFG_037_2560_1792.PNG\n",
      "/home/data/strips_socrates/130/Eurecom_130_picBG_022_2560_1792.PNG\n",
      "/home/data/strips_socrates/130/Eurecom_130_picBG_030_2560_1792.PNG\n",
      "0.0\n",
      "/home/data/strips_socrates/135/Eurecom_135_picBG_042_2304_512.PNG\n",
      "/home/data/strips_socrates/135/Eurecom_135_picBG_023_2304_512.PNG\n",
      "/home/data/strips_socrates/135/Eurecom_135_picBG_032_2304_512.PNG\n",
      "/home/data/strips_socrates/135/Eurecom_135_picFG_031_2304_512.PNG\n",
      "/home/data/strips_socrates/123/Eurecom_123_picBG_028_2304_768.PNG\n",
      "/home/data/strips_socrates/123/Eurecom_123_picBG_021_2304_768.PNG\n",
      "/home/data/strips_socrates/123/Eurecom_123_picFG_036_2304_768.PNG\n",
      "/home/data/strips_socrates/123/Eurecom_123_picFG_039_2304_768.PNG\n",
      "0.0\n",
      "/home/data/strips_socrates/100/Eurecom_100_picBG_006_3840_2048.PNG\n",
      "/home/data/strips_socrates/100/Eurecom_100_picBG_036_3840_2048.PNG\n",
      "/home/data/strips_socrates/100/Eurecom_100_picFG_023_3840_2048.PNG\n",
      "/home/data/strips_socrates/100/Eurecom_100_picBG_044_3840_2048.PNG\n",
      "/home/data/strips_socrates/131/Eurecom_131_picBG_007_1024_0.PNG\n",
      "/home/data/strips_socrates/131/Eurecom_131_picBG_014_1024_0.PNG\n",
      "/home/data/strips_socrates/131/Eurecom_131_picFG_008_1024_0.PNG\n",
      "/home/data/strips_socrates/131/Eurecom_131_picFG_020_1024_0.PNG\n",
      "0.0\n",
      "/home/data/strips_socrates/123/Eurecom_123_picBG_010_1280_2048.PNG\n",
      "/home/data/strips_socrates/123/Eurecom_123_picFG_012_1280_2048.PNG\n",
      "/home/data/strips_socrates/123/Eurecom_123_picFG_021_1280_2048.PNG\n",
      "/home/data/strips_socrates/123/Eurecom_123_picFG_016_1280_2048.PNG\n",
      "/home/data/strips_socrates/115/Eurecom_115_picFG_003_1280_768.PNG\n",
      "/home/data/strips_socrates/115/Eurecom_115_picFG_028_1280_768.PNG\n",
      "/home/data/strips_socrates/115/Eurecom_115_picBG_020_1280_768.PNG\n",
      "/home/data/strips_socrates/115/Eurecom_115_picFG_040_1280_768.PNG\n",
      "0.0\n",
      "/home/data/strips_socrates/107/Eurecom_107_picBG_050_1792_2048.PNG\n",
      "/home/data/strips_socrates/107/Eurecom_107_picFG_034_1792_2048.PNG\n",
      "/home/data/strips_socrates/107/Eurecom_107_picFG_016_1792_2048.PNG\n",
      "/home/data/strips_socrates/107/Eurecom_107_picBG_012_1792_2048.PNG\n",
      "/home/data/strips_socrates/112/Eurecom_112_picBG_016_768_512.PNG\n",
      "/home/data/strips_socrates/112/Eurecom_112_picFG_024_768_512.PNG\n",
      "/home/data/strips_socrates/112/Eurecom_112_picBG_040_768_512.PNG\n",
      "/home/data/strips_socrates/112/Eurecom_112_picBG_028_768_512.PNG\n",
      "1.0\n",
      "/home/data/strips_socrates/119/Eurecom_119_picBG_002_256_1792.PNG\n",
      "/home/data/strips_socrates/119/Eurecom_119_picFG_024_256_1792.PNG\n",
      "/home/data/strips_socrates/119/Eurecom_119_picBG_021_256_1792.PNG\n",
      "/home/data/strips_socrates/119/Eurecom_119_picFG_039_256_1792.PNG\n",
      "/home/data/strips_socrates/119/Eurecom_119_picBG_044_256_1792.PNG\n",
      "/home/data/strips_socrates/119/Eurecom_119_picBG_040_256_1792.PNG\n",
      "/home/data/strips_socrates/119/Eurecom_119_picBG_019_256_1792.PNG\n",
      "/home/data/strips_socrates/119/Eurecom_119_picFG_014_256_1792.PNG\n",
      "1.0\n",
      "/home/data/strips_socrates/108/Eurecom_108_picBG_048_512_0.PNG\n",
      "/home/data/strips_socrates/108/Eurecom_108_picFG_032_512_0.PNG\n",
      "/home/data/strips_socrates/108/Eurecom_108_picFG_019_512_0.PNG\n",
      "/home/data/strips_socrates/108/Eurecom_108_picBG_038_512_0.PNG\n",
      "/home/data/strips_socrates/108/Eurecom_108_picFG_037_512_0.PNG\n",
      "/home/data/strips_socrates/108/Eurecom_108_picBG_009_512_0.PNG\n",
      "/home/data/strips_socrates/108/Eurecom_108_picFG_004_512_0.PNG\n",
      "/home/data/strips_socrates/108/Eurecom_108_picBG_007_512_0.PNG\n",
      "1.0\n",
      "/home/data/strips_socrates/121/Eurecom_121_picBG_007_256_768.PNG\n",
      "/home/data/strips_socrates/121/Eurecom_121_picFG_004_256_768.PNG\n",
      "/home/data/strips_socrates/121/Eurecom_121_picFG_039_256_768.PNG\n",
      "/home/data/strips_socrates/121/Eurecom_121_picBG_021_256_768.PNG\n",
      "/home/data/strips_socrates/121/Eurecom_121_picFG_027_256_768.PNG\n",
      "/home/data/strips_socrates/121/Eurecom_121_picBG_032_256_768.PNG\n",
      "/home/data/strips_socrates/121/Eurecom_121_picBG_015_256_768.PNG\n",
      "/home/data/strips_socrates/121/Eurecom_121_picFG_015_256_768.PNG\n",
      "1.0\n",
      "/home/data/strips_socrates/132/Eurecom_132_picFG_014_3840_768.PNG\n",
      "/home/data/strips_socrates/132/Eurecom_132_picBG_006_3840_768.PNG\n",
      "/home/data/strips_socrates/132/Eurecom_132_picFG_034_3840_768.PNG\n",
      "/home/data/strips_socrates/132/Eurecom_132_picBG_017_3840_768.PNG\n",
      "/home/data/strips_socrates/132/Eurecom_132_picBG_025_3840_768.PNG\n",
      "/home/data/strips_socrates/132/Eurecom_132_picFG_033_3840_768.PNG\n",
      "/home/data/strips_socrates/132/Eurecom_132_picBG_039_3840_768.PNG\n",
      "/home/data/strips_socrates/132/Eurecom_132_picBG_011_3840_768.PNG\n",
      "1.0\n",
      "/home/data/strips_socrates/101/Eurecom_101_picBG_017_3584_1536.PNG\n",
      "/home/data/strips_socrates/101/Eurecom_101_picBG_032_3584_1536.PNG\n",
      "/home/data/strips_socrates/101/Eurecom_101_picBG_027_3584_1536.PNG\n",
      "/home/data/strips_socrates/101/Eurecom_101_picFG_011_3584_1536.PNG\n",
      "/home/data/strips_socrates/101/Eurecom_101_picBG_018_3584_1536.PNG\n",
      "/home/data/strips_socrates/101/Eurecom_101_picFG_001_3584_1536.PNG\n",
      "/home/data/strips_socrates/101/Eurecom_101_picFG_032_3584_1536.PNG\n",
      "/home/data/strips_socrates/101/Eurecom_101_picFG_012_3584_1536.PNG\n"
     ]
    }
   ],
   "source": [
    "for r,s in zip(l, ids):\n",
    "    print(r)\n",
    "    for a in s:\n",
    "        for b in a:\n",
    "            print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(info_data, examples, batch_size, dataset='train'):\n",
    "    while True:\n",
    "        pairs, labels = get_batch(info_data, batch_size, examples, False, dataset=dataset)\n",
    "        yield(pairs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_block(input_shape = (256,256, 1), base_filters=64):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(base_filters, (3,3), input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (3,3)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (5,5)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (5,5)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (7,7)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(base_filters*64, activation='relu'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = sequential_block(base_filters=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 254, 254, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 127, 127, 32)      224       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 125, 125, 128)     36992     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 62, 62, 128)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 58, 58, 128)       409728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 29, 29, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 29, 29, 128)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 128)       409728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 128)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 128)         802944    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 3, 128)         896       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              2361344   \n",
      "=================================================================\n",
      "Total params: 4,024,864\n",
      "Trainable params: 4,022,144\n",
      "Non-trainable params: 2,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape = (4, 256,256,1), base_filters=16):\n",
    "\n",
    "    left_input = layers.Input(input_shape)\n",
    "    right_input = layers.Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    subshape = input_shape[1:4]\n",
    "    model = sequential_block(subshape, base_filters)\n",
    "    \n",
    "    #average noise patterns for image examples\n",
    "    avged_input_l = layers.Average()([left_input[:,k,...] for k in range(input_shape[0])])\n",
    "    avged_input_r = layers.Average()([right_input[:,k,...] for k in range(input_shape[0])])\n",
    "\n",
    "    encoded_l = model(avged_input_l)\n",
    "    encoded_r = model(avged_input_r)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = layers.Dense(1,activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = keras.Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                                      Output Shape                                Param #                 Connected to                                                      \n",
      "========================================================================================================================================================================================================\n",
      "input_1 (InputLayer)                                              [(None, 4, 256, 256, 1)]                    0                                                                                         \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "input_2 (InputLayer)                                              [(None, 4, 256, 256, 1)]                    0                                                                                         \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (SlicingOpLambda)                        (None, 256, 256, 1)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (SlicingOpLambda)                      (None, 256, 256, 1)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (SlicingOpLambda)                      (None, 256, 256, 1)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_3 (SlicingOpLambda)                      (None, 256, 256, 1)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_4 (SlicingOpLambda)                      (None, 256, 256, 1)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_5 (SlicingOpLambda)                      (None, 256, 256, 1)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_6 (SlicingOpLambda)                      (None, 256, 256, 1)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_7 (SlicingOpLambda)                      (None, 256, 256, 1)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "average (Average)                                                 (None, 256, 256, 1)                         0                       tf.__operators__.getitem[0][0]                                    \n",
      "                                                                                                                                      tf.__operators__.getitem_1[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_2[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_3[0][0]                                  \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "average_1 (Average)                                               (None, 256, 256, 1)                         0                       tf.__operators__.getitem_4[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_5[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_6[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_7[0][0]                                  \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)                                         (None, 2560)                                6286760                 average[0][0]                                                     \n",
      "                                                                                                                                      average_1[0][0]                                                   \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "lambda (Lambda)                                                   (None, 2560)                                0                       sequential_1[0][0]                                                \n",
      "                                                                                                                                      sequential_1[1][0]                                                \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "dense_2 (Dense)                                                   (None, 1)                                   2561                    lambda[0][0]                                                      \n",
      "========================================================================================================================================================================================================\n",
      "Total params: 6,289,321\n",
      "Trainable params: 6,285,921\n",
      "Non-trainable params: 3,400\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model(base_filters=40)\n",
    "model.summary(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-6\n",
    "end_learning_rate = 1e-8\n",
    "decay_steps = 12800 * 5\n",
    "lr_schedule = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate, decay_steps, end_learning_rate, power=1\n",
    ")\n",
    "model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        metrics=[keras.metrics.Precision(name='precision'),\n",
    "             keras.metrics.Recall(name='recall'), \"accuracy\"] \n",
    "    )\n",
    "run_name = \"denoised04-dropout-es02\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs/\"+run_name)\n",
    "\n",
    "checkpoint_filepath = \"checkPts/siamese_\"+run_name+\".h5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_best_only=True)\n",
    "\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 500\n",
    "examples = 4\n",
    "steps_epoch = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(generate(info_data, examples, batch_size), \n",
    "         epochs = epochs,\n",
    "         steps_per_epoch= steps_epoch,\n",
    "          callbacks=[tensorboard_callback,\n",
    "                     model_checkpoint_callback,\n",
    "                     early_stopping_callback],\n",
    "          validation_batch_size=16,\n",
    "          validation_data=val_set,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"last-hack02.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"checkPts/siamese_denoised04-dropout-es.h5\") #best model so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation:\n",
    "Simple, just feed pairs from the validation split and see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = get_batch(info_data, 500, 4, False, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(val_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = [y == (p>0) for y, p in zip (val_set_y, preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(preds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[228.  22.]\n",
      " [ 72. 178.]] 0.812 0.712\n"
     ]
    }
   ],
   "source": [
    "cs = np.zeros((2,2))\n",
    "for y,p in zip (val_set[1], preds):\n",
    "    cs[int(y), round(p[0])] += 1 \n",
    "acc = (cs[0,0] + cs [1 , 1]) / np.sum(cs)\n",
    "rec = cs[1,1] / np.sum(cs[1])\n",
    "print(cs, acc, rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended validation: (Not impemented for this model yet)\n",
    "Take two images, and compare strip by strip, then do a majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_batch(info_data, num_whole_images, batch_size):\n",
    "    images = []\n",
    "    split_index = int((len(info_data)-1) * 0.8)\n",
    "    students = [rng.randint(split_index+1, len(info_data)-1) for _ in range(num_whole_images)] \n",
    "    labels = np.zeros((num_whole_images, ))\n",
    "    labels[num_whole_images//2:] = 1\n",
    "    id_pairs_img = []\n",
    "    for j in range(num_whole_images):\n",
    "        pairs = [np.zeros((batch_size, STRIP_SIZE, STRIP_SIZE, 3)) for i in range(2)]\n",
    "        imgs = [rng.randint(0, len(info_data[i])-1) for i in students]\n",
    "        std = students[j]\n",
    "        img = imgs[j]\n",
    "        id_pairs = []\n",
    "        std2 = (std + rng.randint(1, len(info_data)-1)) % len(info_data)\n",
    "        if j >= num_whole_images // 2:   \n",
    "            img2 = (img + rng.randint(1, len(info_data[std])-1)) % len(info_data[std])\n",
    "            std2 = std\n",
    "        else:\n",
    "            std2 = (std + rng.randint(1, len(info_data)-1)) % len(info_data)\n",
    "            img2 = rng.randint(0, len(info_data[std2])-1)\n",
    "        if batch_size > len(info_data[std][img][1]):\n",
    "            print(\"a\", info_data[std][0][0])\n",
    "            std = std+1\n",
    "            \n",
    "        if batch_size > len(info_data[std2][img2][1]):\n",
    "            print(\"b\", info_data[std2][0][0])\n",
    "            std2 = std+1\n",
    "            \n",
    "        for i in range(batch_size):    \n",
    "            strip1 = info_data[std][img][1][i]\n",
    "            strip2 = info_data[std2][img2][1][i]\n",
    "#             print(strip1, strip2)\n",
    "            pairs[0][i,:,:,:] = cv2.imread(strip1)/255\n",
    "            pairs[1][i,:,:,:] = cv2.imread(strip2)/255\n",
    "            id_pairs.append((strip1, strip2))\n",
    "        images.append(pairs)\n",
    "        id_pairs_img.append(id_pairs)\n",
    "    \n",
    "    return images, labels, id_pairs_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_i, t_l, t_d = get_val_batch(info_data, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = np.zeros((2,2))\n",
    "cs_all = np.zeros((2,2))\n",
    "for i, l, d in zip(t_i, t_l, t_d):\n",
    "    preds = model.predict(i)\n",
    "    for p in preds:\n",
    "        cs_all[int(l), round(p[0])] += 1\n",
    "#     print(int(l), round(np.mean(preds)), (np.mean(preds)))\n",
    "    fp = sum([p[0] > 0.5 for p in preds]) > len(preds)//2 \n",
    "    cs[int(l), int(fp)] +=1\n",
    "#     print(l, d[0])\n",
    "\n",
    "acc = (cs[0,0] + cs [1 , 1]) / np.sum(cs)\n",
    "rec = cs[1,1] / np.sum(cs[1])\n",
    "print(cs, acc, rec)\n",
    "cs = cs_all\n",
    "acc = (cs[0,0] + cs [1 , 1]) / np.sum(cs)\n",
    "rec = cs[1,1] / np.sum(cs[1])\n",
    "print(cs, acc, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
