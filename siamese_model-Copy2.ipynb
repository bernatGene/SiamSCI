{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese network implementation with the strips dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random as rng\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data = []\n",
    "with open('/home/data/strips_socrates/dataset_info.json') as json_file: \n",
    "    info_data = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIP_SIZE = 256\n",
    "SUB_STRIP_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(info_data, batch_size, with_id=False, dataset = 'train'):\n",
    "    pairs = [np.zeros((batch_size, STRIP_SIZE, STRIP_SIZE, 3)) for i in range(2)]\n",
    "    labels = np.zeros((batch_size, ))\n",
    "    labels[batch_size//2:] = 1\n",
    "    split_index = int((len(info_data)-1) * 0.8)\n",
    "    if dataset == 'train':\n",
    "        students = [rng.randint(0, split_index) for _ in range(batch_size)] \n",
    "    elif dataset == 'whole':\n",
    "        students = [rng.randint(0, (len(info_data)-1)) for _ in range(batch_size)] \n",
    "    else:\n",
    "        students = [rng.randint(split_index+1, len(info_data)-1) for _ in range(batch_size)] \n",
    "    imgs = [rng.randint(0, len(info_data[i])-1) for i in students]\n",
    "    strips_loc = [rng.randint(0, len(info_data[i][0][1])-1) for i in students]\n",
    "    id_pairs = []\n",
    "    for i in range(batch_size):\n",
    "        std = students[i]\n",
    "        img = imgs[i]\n",
    "        srl = strips_loc[i]\n",
    "        strip1 = info_data[std][img][1][srl]\n",
    "        strip2 = \"\"\n",
    "        if i >= batch_size // 2:\n",
    "            img2 = (img + rng.randint(1, len(info_data[std])-1)) % len(info_data[std])\n",
    "            strip2 = info_data[std][img2][1][srl]\n",
    "        else:\n",
    "            std2 = (std + rng.randint(1, len(info_data)-1)) % len(info_data)\n",
    "            img2 = rng.randint(0, len(info_data[std2])-1)\n",
    "            srl2 = rng.randint(0, len(info_data[std2][0][1])-1)\n",
    "            strip2 = info_data[std2][img2][1][srl2]\n",
    "        pairs[0][i,:,:,:] = cv2.imread(strip1)/255\n",
    "        pairs[1][i,:,:,:] = cv2.imread(strip2)/255\n",
    "        id_pairs.append((strip1, strip2))\n",
    "    if with_id:\n",
    "        return pairs, labels, id_pairs\n",
    "    else:\n",
    "        return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,l,ids = get_batch(info_data, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in zip(l, ids):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(info_data, batch_size, dataset='train'):\n",
    "    while True:\n",
    "        pairs, labels = get_batch(info_data, batch_size, False, dataset=dataset)\n",
    "        yield(pairs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_block(input_shape = (256,256,3), base_filters=64):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    model = keras.applications.EfficientNetB1(include_top=False, input_tensor=inputs, weights=None)\n",
    "\n",
    "    \n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    \n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(1024, activation=\"softmax\", name=\"pred\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = sequential_block(base_filters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape = (256,256,3), base_filters=16):\n",
    "\n",
    "    left_input = layers.Input(input_shape)\n",
    "    right_input = layers.Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = sequential_block(input_shape, base_filters)\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = layers.Dense(1,activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = keras.Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model_ensemble(input_shape = (STRIP_SIZE,STRIP_SIZE,3), base_filters=16):\n",
    "\n",
    "    left_input = layers.Input(input_shape)\n",
    "    left_A = layers.Cropping2D(cropping=((0,SUB_STRIP_SIZE), (0,SUB_STRIP_SIZE)))(left_input)\n",
    "    left_B = layers.Cropping2D(cropping=((SUB_STRIP_SIZE,0), (0,SUB_STRIP_SIZE)))(left_input)\n",
    "    left_C = layers.Cropping2D(cropping=((0,SUB_STRIP_SIZE), (SUB_STRIP_SIZE,0)))(left_input)\n",
    "    left_D = layers.Cropping2D(cropping=((SUB_STRIP_SIZE,0), (SUB_STRIP_SIZE,0)))(left_input)\n",
    "    right_input = layers.Input(input_shape)\n",
    "    right_A = layers.Cropping2D(cropping=((0,SUB_STRIP_SIZE), (0,SUB_STRIP_SIZE)))(right_input)\n",
    "    right_B = layers.Cropping2D(cropping=((SUB_STRIP_SIZE,0), (0,SUB_STRIP_SIZE)))(right_input)\n",
    "    right_C = layers.Cropping2D(cropping=((0,SUB_STRIP_SIZE), (SUB_STRIP_SIZE,0)))(right_input)\n",
    "    right_D = layers.Cropping2D(cropping=((SUB_STRIP_SIZE,0), (SUB_STRIP_SIZE,0)))(right_input)\n",
    "    \n",
    "    \n",
    "    siam = get_siamese_model((SUB_STRIP_SIZE,SUB_STRIP_SIZE,3), base_filters=base_filters)\n",
    "    out_A = siam([left_A, right_A]) \n",
    "    out_B = siam([left_B, right_B]) \n",
    "    out_C = siam([left_C, right_C]) \n",
    "    out_D = siam([left_D, right_D]) \n",
    "    \n",
    "    prediction = layers.Average()([out_A, out_B, out_C, out_D])\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = keras.Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 1024)         7892103     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1024)         0           model[0][0]                      \n",
      "                                                                 model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,893,128\n",
      "Trainable params: 7,828,513\n",
      "Non-trainable params: 64,615\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model(base_filters=64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-6\n",
    "end_learning_rate = 1e-7\n",
    "decay_steps = 12800\n",
    "lr_schedule = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate, decay_steps, end_learning_rate, power=0.5\n",
    ")\n",
    "model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        metrics=[keras.metrics.Precision(name='precision'),\n",
    "             keras.metrics.Recall(name='recall'), \"accuracy\"] \n",
    "    )\n",
    "run_name = \"enet-test03-withval-propper\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs/\"+run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 280\n",
    "steps_epoch = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/280\n",
      "128/128 [==============================] - 109s 707ms/step - loss: 0.6870 - precision: 0.5643 - recall: 0.5540 - accuracy: 0.5627 - val_loss: 0.6862 - val_precision: 0.5701 - val_recall: 0.6100 - val_accuracy: 0.5750\n",
      "Epoch 2/280\n",
      "128/128 [==============================] - 84s 658ms/step - loss: 0.6806 - precision: 0.5989 - recall: 0.6494 - accuracy: 0.6075 - val_loss: 0.6906 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 3/280\n",
      "128/128 [==============================] - 85s 660ms/step - loss: 0.6855 - precision: 0.5788 - recall: 0.5805 - accuracy: 0.5786 - val_loss: 0.6870 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 4/280\n",
      "128/128 [==============================] - 84s 660ms/step - loss: 0.6914 - precision: 0.5218 - recall: 0.5747 - accuracy: 0.5225 - val_loss: 0.6901 - val_precision: 0.5455 - val_recall: 0.5400 - val_accuracy: 0.5450\n",
      "Epoch 5/280\n",
      "128/128 [==============================] - 84s 659ms/step - loss: 0.6878 - precision: 0.5568 - recall: 0.5555 - accuracy: 0.5568 - val_loss: 0.6890 - val_precision: 0.5556 - val_recall: 0.5500 - val_accuracy: 0.5550\n",
      "Epoch 6/280\n",
      "128/128 [==============================] - 85s 660ms/step - loss: 0.6885 - precision: 0.5515 - recall: 0.5645 - accuracy: 0.5533 - val_loss: 0.6901 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 7/280\n",
      "128/128 [==============================] - 85s 661ms/step - loss: 0.6850 - precision: 0.5701 - recall: 0.5946 - accuracy: 0.5732 - val_loss: 0.6868 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 8/280\n",
      "128/128 [==============================] - 85s 662ms/step - loss: 0.6868 - precision: 0.5503 - recall: 0.5368 - accuracy: 0.5497 - val_loss: 0.6873 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 9/280\n",
      "128/128 [==============================] - 85s 661ms/step - loss: 0.6820 - precision: 0.5950 - recall: 0.5508 - accuracy: 0.5882 - val_loss: 0.6875 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 10/280\n",
      "128/128 [==============================] - 85s 664ms/step - loss: 0.6883 - precision: 0.5413 - recall: 0.5754 - accuracy: 0.5448 - val_loss: 0.6855 - val_precision: 0.5741 - val_recall: 0.6200 - val_accuracy: 0.5800\n",
      "Epoch 11/280\n",
      "128/128 [==============================] - 85s 663ms/step - loss: 0.6827 - precision: 0.5901 - recall: 0.6182 - accuracy: 0.5943 - val_loss: 0.6860 - val_precision: 0.5755 - val_recall: 0.6100 - val_accuracy: 0.5800\n",
      "Epoch 12/280\n",
      "128/128 [==============================] - 85s 665ms/step - loss: 0.6828 - precision: 0.5940 - recall: 0.6345 - accuracy: 0.6006 - val_loss: 0.6896 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 13/280\n",
      "128/128 [==============================] - 85s 665ms/step - loss: 0.6835 - precision: 0.5770 - recall: 0.6203 - accuracy: 0.5835 - val_loss: 0.6886 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 14/280\n",
      "128/128 [==============================] - 85s 665ms/step - loss: 0.6864 - precision: 0.5489 - recall: 0.5745 - accuracy: 0.5527 - val_loss: 0.6885 - val_precision: 0.5481 - val_recall: 0.5700 - val_accuracy: 0.5500\n",
      "Epoch 15/280\n",
      "128/128 [==============================] - 85s 668ms/step - loss: 0.6853 - precision: 0.5610 - recall: 0.6252 - accuracy: 0.5677 - val_loss: 0.6871 - val_precision: 0.5631 - val_recall: 0.5800 - val_accuracy: 0.5650\n",
      "Epoch 16/280\n",
      "128/128 [==============================] - 86s 671ms/step - loss: 0.6876 - precision: 0.5566 - recall: 0.5953 - accuracy: 0.5604 - val_loss: 0.6885 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 17/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6887 - precision: 0.5466 - recall: 0.5570 - accuracy: 0.5477 - val_loss: 0.6884 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 18/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6841 - precision: 0.5745 - recall: 0.6127 - accuracy: 0.5791 - val_loss: 0.6872 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 19/280\n",
      "128/128 [==============================] - 86s 671ms/step - loss: 0.6844 - precision: 0.5822 - recall: 0.6009 - accuracy: 0.5849 - val_loss: 0.6866 - val_precision: 0.5755 - val_recall: 0.6100 - val_accuracy: 0.5800\n",
      "Epoch 20/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6849 - precision: 0.5738 - recall: 0.6418 - accuracy: 0.5826 - val_loss: 0.6901 - val_precision: 0.5481 - val_recall: 0.5700 - val_accuracy: 0.5500\n",
      "Epoch 21/280\n",
      "128/128 [==============================] - 86s 672ms/step - loss: 0.6856 - precision: 0.5670 - recall: 0.5611 - accuracy: 0.5658 - val_loss: 0.6875 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 22/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6805 - precision: 0.6131 - recall: 0.6486 - accuracy: 0.6191 - val_loss: 0.6876 - val_precision: 0.5514 - val_recall: 0.5900 - val_accuracy: 0.5550\n",
      "Epoch 23/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6843 - precision: 0.5826 - recall: 0.6147 - accuracy: 0.5866 - val_loss: 0.6871 - val_precision: 0.5673 - val_recall: 0.5900 - val_accuracy: 0.5700\n",
      "Epoch 24/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6836 - precision: 0.5839 - recall: 0.6161 - accuracy: 0.5883 - val_loss: 0.6860 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 25/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6831 - precision: 0.5957 - recall: 0.6205 - accuracy: 0.5996 - val_loss: 0.6908 - val_precision: 0.5437 - val_recall: 0.5600 - val_accuracy: 0.5450\n",
      "Epoch 26/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6813 - precision: 0.5953 - recall: 0.6222 - accuracy: 0.6004 - val_loss: 0.6869 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 27/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6886 - precision: 0.5553 - recall: 0.5343 - accuracy: 0.5544 - val_loss: 0.6871 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 28/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6847 - precision: 0.5711 - recall: 0.6322 - accuracy: 0.5782 - val_loss: 0.6888 - val_precision: 0.5534 - val_recall: 0.5700 - val_accuracy: 0.5550\n",
      "Epoch 29/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6824 - precision: 0.5935 - recall: 0.5611 - accuracy: 0.5856 - val_loss: 0.6854 - val_precision: 0.5727 - val_recall: 0.6300 - val_accuracy: 0.5800\n",
      "Epoch 30/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6881 - precision: 0.5420 - recall: 0.5342 - accuracy: 0.5415 - val_loss: 0.6849 - val_precision: 0.5701 - val_recall: 0.6100 - val_accuracy: 0.5750\n",
      "Epoch 31/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6829 - precision: 0.5816 - recall: 0.5854 - accuracy: 0.5829 - val_loss: 0.6900 - val_precision: 0.5340 - val_recall: 0.5500 - val_accuracy: 0.5350\n",
      "Epoch 32/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6842 - precision: 0.5833 - recall: 0.5827 - accuracy: 0.5829 - val_loss: 0.6908 - val_precision: 0.5400 - val_recall: 0.5400 - val_accuracy: 0.5400\n",
      "Epoch 33/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6865 - precision: 0.5579 - recall: 0.5652 - accuracy: 0.5581 - val_loss: 0.6893 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 34/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6854 - precision: 0.5699 - recall: 0.6233 - accuracy: 0.5764 - val_loss: 0.6875 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 35/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6878 - precision: 0.5448 - recall: 0.6225 - accuracy: 0.5510 - val_loss: 0.6867 - val_precision: 0.5648 - val_recall: 0.6100 - val_accuracy: 0.5700\n",
      "Epoch 36/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6891 - precision: 0.5365 - recall: 0.6116 - accuracy: 0.5423 - val_loss: 0.6889 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 37/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6855 - precision: 0.5622 - recall: 0.5879 - accuracy: 0.5686 - val_loss: 0.6865 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 38/280\n",
      "128/128 [==============================] - 86s 672ms/step - loss: 0.6859 - precision: 0.5638 - recall: 0.5678 - accuracy: 0.5642 - val_loss: 0.6892 - val_precision: 0.5455 - val_recall: 0.5400 - val_accuracy: 0.5450\n",
      "Epoch 39/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6822 - precision: 0.5873 - recall: 0.6107 - accuracy: 0.5944 - val_loss: 0.6897 - val_precision: 0.5481 - val_recall: 0.5700 - val_accuracy: 0.5500\n",
      "Epoch 40/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6857 - precision: 0.5699 - recall: 0.6005 - accuracy: 0.5726 - val_loss: 0.6858 - val_precision: 0.5648 - val_recall: 0.6100 - val_accuracy: 0.5700\n",
      "Epoch 41/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6832 - precision: 0.5884 - recall: 0.6394 - accuracy: 0.5957 - val_loss: 0.6889 - val_precision: 0.5500 - val_recall: 0.5500 - val_accuracy: 0.5500\n",
      "Epoch 42/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6855 - precision: 0.5512 - recall: 0.6312 - accuracy: 0.5586 - val_loss: 0.6867 - val_precision: 0.5607 - val_recall: 0.6000 - val_accuracy: 0.5650\n",
      "Epoch 43/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6875 - precision: 0.5593 - recall: 0.5666 - accuracy: 0.5576 - val_loss: 0.6918 - val_precision: 0.5294 - val_recall: 0.5400 - val_accuracy: 0.5300\n",
      "Epoch 44/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6909 - precision: 0.5242 - recall: 0.5826 - accuracy: 0.5274 - val_loss: 0.6875 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 45/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6874 - precision: 0.5506 - recall: 0.5833 - accuracy: 0.5531 - val_loss: 0.6902 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 46/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6830 - precision: 0.5820 - recall: 0.6310 - accuracy: 0.5892 - val_loss: 0.6895 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 47/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6818 - precision: 0.5910 - recall: 0.6499 - accuracy: 0.6003 - val_loss: 0.6870 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 48/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6904 - precision: 0.5367 - recall: 0.5841 - accuracy: 0.5359 - val_loss: 0.6867 - val_precision: 0.5741 - val_recall: 0.6200 - val_accuracy: 0.5800\n",
      "Epoch 49/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6904 - precision: 0.5289 - recall: 0.6266 - accuracy: 0.5332 - val_loss: 0.6910 - val_precision: 0.5347 - val_recall: 0.5400 - val_accuracy: 0.5350\n",
      "Epoch 50/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6821 - precision: 0.5869 - recall: 0.6122 - accuracy: 0.5909 - val_loss: 0.6908 - val_precision: 0.5333 - val_recall: 0.5600 - val_accuracy: 0.5350\n",
      "Epoch 51/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6839 - precision: 0.5905 - recall: 0.6082 - accuracy: 0.5939 - val_loss: 0.6859 - val_precision: 0.5701 - val_recall: 0.6100 - val_accuracy: 0.5750\n",
      "Epoch 52/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6854 - precision: 0.5653 - recall: 0.6065 - accuracy: 0.5697 - val_loss: 0.6906 - val_precision: 0.5238 - val_recall: 0.5500 - val_accuracy: 0.5250\n",
      "Epoch 53/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6841 - precision: 0.5712 - recall: 0.6393 - accuracy: 0.5791 - val_loss: 0.6865 - val_precision: 0.5701 - val_recall: 0.6100 - val_accuracy: 0.5750\n",
      "Epoch 54/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6834 - precision: 0.5783 - recall: 0.5959 - accuracy: 0.5800 - val_loss: 0.6910 - val_precision: 0.5400 - val_recall: 0.5400 - val_accuracy: 0.5400\n",
      "Epoch 55/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6868 - precision: 0.5513 - recall: 0.5960 - accuracy: 0.5551 - val_loss: 0.6866 - val_precision: 0.5607 - val_recall: 0.6000 - val_accuracy: 0.5650\n",
      "Epoch 56/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6853 - precision: 0.5759 - recall: 0.6002 - accuracy: 0.5791 - val_loss: 0.6900 - val_precision: 0.5288 - val_recall: 0.5500 - val_accuracy: 0.5300\n",
      "Epoch 57/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6862 - precision: 0.5603 - recall: 0.5795 - accuracy: 0.5624 - val_loss: 0.6910 - val_precision: 0.5294 - val_recall: 0.5400 - val_accuracy: 0.5300\n",
      "Epoch 58/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6842 - precision: 0.5914 - recall: 0.5606 - accuracy: 0.5846 - val_loss: 0.6873 - val_precision: 0.5514 - val_recall: 0.5900 - val_accuracy: 0.5550\n",
      "Epoch 59/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6871 - precision: 0.5568 - recall: 0.5792 - accuracy: 0.5591 - val_loss: 0.6887 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 60/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6855 - precision: 0.5639 - recall: 0.5973 - accuracy: 0.5675 - val_loss: 0.6877 - val_precision: 0.5607 - val_recall: 0.6000 - val_accuracy: 0.5650\n",
      "Epoch 61/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6846 - precision: 0.5624 - recall: 0.6443 - accuracy: 0.5701 - val_loss: 0.6906 - val_precision: 0.5347 - val_recall: 0.5400 - val_accuracy: 0.5350\n",
      "Epoch 62/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6829 - precision: 0.5754 - recall: 0.6087 - accuracy: 0.5793 - val_loss: 0.6872 - val_precision: 0.5607 - val_recall: 0.6000 - val_accuracy: 0.5650\n",
      "Epoch 63/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6822 - precision: 0.5951 - recall: 0.6618 - accuracy: 0.6052 - val_loss: 0.6861 - val_precision: 0.5648 - val_recall: 0.6100 - val_accuracy: 0.5700\n",
      "Epoch 64/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6813 - precision: 0.6018 - recall: 0.6234 - accuracy: 0.6044 - val_loss: 0.6884 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 65/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6834 - precision: 0.5752 - recall: 0.6163 - accuracy: 0.5800 - val_loss: 0.6906 - val_precision: 0.5340 - val_recall: 0.5500 - val_accuracy: 0.5350\n",
      "Epoch 66/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6870 - precision: 0.5607 - recall: 0.6177 - accuracy: 0.5665 - val_loss: 0.6880 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 67/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6838 - precision: 0.5853 - recall: 0.6243 - accuracy: 0.5906 - val_loss: 0.6870 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 68/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6864 - precision: 0.5595 - recall: 0.5542 - accuracy: 0.5598 - val_loss: 0.6895 - val_precision: 0.5481 - val_recall: 0.5700 - val_accuracy: 0.5500\n",
      "Epoch 69/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6817 - precision: 0.5925 - recall: 0.5875 - accuracy: 0.5870 - val_loss: 0.6863 - val_precision: 0.5701 - val_recall: 0.6100 - val_accuracy: 0.5750\n",
      "Epoch 70/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6853 - precision: 0.5672 - recall: 0.5811 - accuracy: 0.5688 - val_loss: 0.6900 - val_precision: 0.5455 - val_recall: 0.5400 - val_accuracy: 0.5450\n",
      "Epoch 71/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6884 - precision: 0.5465 - recall: 0.5968 - accuracy: 0.5491 - val_loss: 0.6878 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 72/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6877 - precision: 0.5454 - recall: 0.5793 - accuracy: 0.5480 - val_loss: 0.6878 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 73/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6882 - precision: 0.5459 - recall: 0.5652 - accuracy: 0.5473 - val_loss: 0.6904 - val_precision: 0.5429 - val_recall: 0.5700 - val_accuracy: 0.5450\n",
      "Epoch 74/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6891 - precision: 0.5446 - recall: 0.5743 - accuracy: 0.5459 - val_loss: 0.6874 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 75/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6924 - precision: 0.5212 - recall: 0.5418 - accuracy: 0.5223 - val_loss: 0.6907 - val_precision: 0.5333 - val_recall: 0.5600 - val_accuracy: 0.5350\n",
      "Epoch 76/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6858 - precision: 0.5638 - recall: 0.5762 - accuracy: 0.5646 - val_loss: 0.6879 - val_precision: 0.5514 - val_recall: 0.5900 - val_accuracy: 0.5550\n",
      "Epoch 77/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6840 - precision: 0.5786 - recall: 0.6103 - accuracy: 0.5828 - val_loss: 0.6890 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 78/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6879 - precision: 0.5487 - recall: 0.6091 - accuracy: 0.5537 - val_loss: 0.6906 - val_precision: 0.5385 - val_recall: 0.5600 - val_accuracy: 0.5400\n",
      "Epoch 79/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6816 - precision: 0.5950 - recall: 0.6449 - accuracy: 0.6031 - val_loss: 0.6875 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 80/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6854 - precision: 0.5606 - recall: 0.6127 - accuracy: 0.5649 - val_loss: 0.6863 - val_precision: 0.5794 - val_recall: 0.6200 - val_accuracy: 0.5850\n",
      "Epoch 81/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6903 - precision: 0.5329 - recall: 0.5575 - accuracy: 0.5346 - val_loss: 0.6889 - val_precision: 0.5481 - val_recall: 0.5700 - val_accuracy: 0.5500\n",
      "Epoch 82/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6840 - precision: 0.5745 - recall: 0.6027 - accuracy: 0.5782 - val_loss: 0.6851 - val_precision: 0.5794 - val_recall: 0.6200 - val_accuracy: 0.5850\n",
      "Epoch 83/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6872 - precision: 0.5505 - recall: 0.5833 - accuracy: 0.5529 - val_loss: 0.6902 - val_precision: 0.5534 - val_recall: 0.5700 - val_accuracy: 0.5550\n",
      "Epoch 84/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6837 - precision: 0.5864 - recall: 0.6434 - accuracy: 0.5932 - val_loss: 0.6879 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 85/280\n",
      "128/128 [==============================] - 87s 678ms/step - loss: 0.6808 - precision: 0.6032 - recall: 0.6191 - accuracy: 0.6061 - val_loss: 0.6877 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 86/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6809 - precision: 0.5922 - recall: 0.6448 - accuracy: 0.6000 - val_loss: 0.6851 - val_precision: 0.5849 - val_recall: 0.6200 - val_accuracy: 0.5900\n",
      "Epoch 87/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6850 - precision: 0.5755 - recall: 0.5596 - accuracy: 0.5735 - val_loss: 0.6859 - val_precision: 0.5755 - val_recall: 0.6100 - val_accuracy: 0.5800\n",
      "Epoch 88/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6871 - precision: 0.5532 - recall: 0.6048 - accuracy: 0.5584 - val_loss: 0.6882 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 89/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6883 - precision: 0.5465 - recall: 0.5745 - accuracy: 0.5487 - val_loss: 0.6907 - val_precision: 0.5392 - val_recall: 0.5500 - val_accuracy: 0.5400\n",
      "Epoch 90/280\n",
      "128/128 [==============================] - 86s 672ms/step - loss: 0.6905 - precision: 0.5235 - recall: 0.5538 - accuracy: 0.5237 - val_loss: 0.6910 - val_precision: 0.5392 - val_recall: 0.5500 - val_accuracy: 0.5400\n",
      "Epoch 91/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6845 - precision: 0.5750 - recall: 0.6266 - accuracy: 0.5821 - val_loss: 0.6878 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 92/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6895 - precision: 0.5324 - recall: 0.5681 - accuracy: 0.5339 - val_loss: 0.6875 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 93/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6854 - precision: 0.5606 - recall: 0.5645 - accuracy: 0.5615 - val_loss: 0.6862 - val_precision: 0.5769 - val_recall: 0.6000 - val_accuracy: 0.5800\n",
      "Epoch 94/280\n",
      "128/128 [==============================] - 87s 678ms/step - loss: 0.6899 - precision: 0.5426 - recall: 0.5539 - accuracy: 0.5440 - val_loss: 0.6874 - val_precision: 0.5686 - val_recall: 0.5800 - val_accuracy: 0.5700\n",
      "Epoch 95/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6863 - precision: 0.5557 - recall: 0.6244 - accuracy: 0.5606 - val_loss: 0.6876 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 96/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6870 - precision: 0.5685 - recall: 0.5644 - accuracy: 0.5656 - val_loss: 0.6880 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 97/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6826 - precision: 0.6007 - recall: 0.6075 - accuracy: 0.6024 - val_loss: 0.6879 - val_precision: 0.5514 - val_recall: 0.5900 - val_accuracy: 0.5550\n",
      "Epoch 98/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6867 - precision: 0.5514 - recall: 0.5730 - accuracy: 0.5530 - val_loss: 0.6850 - val_precision: 0.5888 - val_recall: 0.6300 - val_accuracy: 0.5950\n",
      "Epoch 99/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6864 - precision: 0.5628 - recall: 0.5791 - accuracy: 0.5651 - val_loss: 0.6849 - val_precision: 0.5849 - val_recall: 0.6200 - val_accuracy: 0.5900\n",
      "Epoch 100/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6869 - precision: 0.5610 - recall: 0.5547 - accuracy: 0.5598 - val_loss: 0.6884 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 101/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6869 - precision: 0.5571 - recall: 0.6303 - accuracy: 0.5627 - val_loss: 0.6889 - val_precision: 0.5385 - val_recall: 0.5600 - val_accuracy: 0.5400\n",
      "Epoch 102/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6867 - precision: 0.5453 - recall: 0.5805 - accuracy: 0.5478 - val_loss: 0.6868 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 103/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6881 - precision: 0.5582 - recall: 0.5526 - accuracy: 0.5565 - val_loss: 0.6869 - val_precision: 0.5673 - val_recall: 0.5900 - val_accuracy: 0.5700\n",
      "Epoch 104/280\n",
      "128/128 [==============================] - 87s 679ms/step - loss: 0.6849 - precision: 0.5696 - recall: 0.6123 - accuracy: 0.5739 - val_loss: 0.6914 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 105/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6825 - precision: 0.5829 - recall: 0.5859 - accuracy: 0.5821 - val_loss: 0.6885 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 106/280\n",
      "128/128 [==============================] - 86s 672ms/step - loss: 0.6862 - precision: 0.5688 - recall: 0.5817 - accuracy: 0.5685 - val_loss: 0.6872 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 107/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6844 - precision: 0.5781 - recall: 0.6232 - accuracy: 0.5843 - val_loss: 0.6875 - val_precision: 0.5607 - val_recall: 0.6000 - val_accuracy: 0.5650\n",
      "Epoch 108/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6889 - precision: 0.5416 - recall: 0.6011 - accuracy: 0.5460 - val_loss: 0.6888 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 109/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6846 - precision: 0.5774 - recall: 0.6257 - accuracy: 0.5836 - val_loss: 0.6910 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 110/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6836 - precision: 0.5842 - recall: 0.6234 - accuracy: 0.5902 - val_loss: 0.6914 - val_precision: 0.5392 - val_recall: 0.5500 - val_accuracy: 0.5400\n",
      "Epoch 111/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6863 - precision: 0.5693 - recall: 0.6118 - accuracy: 0.5749 - val_loss: 0.6897 - val_precision: 0.5500 - val_recall: 0.5500 - val_accuracy: 0.5500\n",
      "Epoch 112/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6877 - precision: 0.5535 - recall: 0.5873 - accuracy: 0.5562 - val_loss: 0.6850 - val_precision: 0.5833 - val_recall: 0.6300 - val_accuracy: 0.5900\n",
      "Epoch 113/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6818 - precision: 0.5879 - recall: 0.6243 - accuracy: 0.5933 - val_loss: 0.6872 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 114/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6851 - precision: 0.5678 - recall: 0.6017 - accuracy: 0.5716 - val_loss: 0.6875 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 115/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6851 - precision: 0.5713 - recall: 0.5665 - accuracy: 0.5696 - val_loss: 0.6854 - val_precision: 0.5849 - val_recall: 0.6200 - val_accuracy: 0.5900\n",
      "Epoch 116/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6850 - precision: 0.5691 - recall: 0.6063 - accuracy: 0.5738 - val_loss: 0.6912 - val_precision: 0.5288 - val_recall: 0.5500 - val_accuracy: 0.5300\n",
      "Epoch 117/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6842 - precision: 0.5768 - recall: 0.5854 - accuracy: 0.5768 - val_loss: 0.6901 - val_precision: 0.5545 - val_recall: 0.5600 - val_accuracy: 0.5550\n",
      "Epoch 118/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6840 - precision: 0.5760 - recall: 0.5917 - accuracy: 0.5778 - val_loss: 0.6874 - val_precision: 0.5631 - val_recall: 0.5800 - val_accuracy: 0.5650\n",
      "Epoch 119/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6843 - precision: 0.5810 - recall: 0.6007 - accuracy: 0.5837 - val_loss: 0.6890 - val_precision: 0.5588 - val_recall: 0.5700 - val_accuracy: 0.5600\n",
      "Epoch 120/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6869 - precision: 0.5569 - recall: 0.5548 - accuracy: 0.5569 - val_loss: 0.6888 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 121/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6846 - precision: 0.5626 - recall: 0.5849 - accuracy: 0.5653 - val_loss: 0.6916 - val_precision: 0.5294 - val_recall: 0.5400 - val_accuracy: 0.5300\n",
      "Epoch 122/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6925 - precision: 0.5143 - recall: 0.5319 - accuracy: 0.5174 - val_loss: 0.6867 - val_precision: 0.5701 - val_recall: 0.6100 - val_accuracy: 0.5750\n",
      "Epoch 123/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6866 - precision: 0.5581 - recall: 0.5852 - accuracy: 0.5627 - val_loss: 0.6901 - val_precision: 0.5400 - val_recall: 0.5400 - val_accuracy: 0.5400\n",
      "Epoch 124/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6849 - precision: 0.5679 - recall: 0.6342 - accuracy: 0.5766 - val_loss: 0.6856 - val_precision: 0.5701 - val_recall: 0.6100 - val_accuracy: 0.5750\n",
      "Epoch 125/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6880 - precision: 0.5352 - recall: 0.5561 - accuracy: 0.5376 - val_loss: 0.6890 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 126/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6854 - precision: 0.5590 - recall: 0.6046 - accuracy: 0.5642 - val_loss: 0.6883 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 127/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6892 - precision: 0.5213 - recall: 0.5611 - accuracy: 0.5251 - val_loss: 0.6902 - val_precision: 0.5437 - val_recall: 0.5600 - val_accuracy: 0.5450\n",
      "Epoch 128/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6863 - precision: 0.5725 - recall: 0.5904 - accuracy: 0.5749 - val_loss: 0.6875 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 129/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6877 - precision: 0.5554 - recall: 0.5726 - accuracy: 0.5572 - val_loss: 0.6869 - val_precision: 0.5648 - val_recall: 0.6100 - val_accuracy: 0.5700\n",
      "Epoch 130/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6892 - precision: 0.5489 - recall: 0.5562 - accuracy: 0.5498 - val_loss: 0.6880 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 131/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6801 - precision: 0.6123 - recall: 0.6066 - accuracy: 0.6113 - val_loss: 0.6856 - val_precision: 0.5849 - val_recall: 0.6200 - val_accuracy: 0.5900\n",
      "Epoch 132/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6860 - precision: 0.5636 - recall: 0.5691 - accuracy: 0.5635 - val_loss: 0.6849 - val_precision: 0.5833 - val_recall: 0.6300 - val_accuracy: 0.5900\n",
      "Epoch 133/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6869 - precision: 0.5578 - recall: 0.5599 - accuracy: 0.5587 - val_loss: 0.6859 - val_precision: 0.5741 - val_recall: 0.6200 - val_accuracy: 0.5800\n",
      "Epoch 134/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6871 - precision: 0.5524 - recall: 0.5602 - accuracy: 0.5537 - val_loss: 0.6889 - val_precision: 0.5481 - val_recall: 0.5700 - val_accuracy: 0.5500\n",
      "Epoch 135/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6866 - precision: 0.5589 - recall: 0.6118 - accuracy: 0.5646 - val_loss: 0.6880 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 136/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6879 - precision: 0.5479 - recall: 0.5881 - accuracy: 0.5516 - val_loss: 0.6884 - val_precision: 0.5545 - val_recall: 0.5600 - val_accuracy: 0.5550\n",
      "Epoch 137/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6846 - precision: 0.5648 - recall: 0.5972 - accuracy: 0.5692 - val_loss: 0.6864 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 138/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6886 - precision: 0.5464 - recall: 0.5980 - accuracy: 0.5505 - val_loss: 0.6868 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 139/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6884 - precision: 0.5520 - recall: 0.5485 - accuracy: 0.5518 - val_loss: 0.6898 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 140/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6899 - precision: 0.5371 - recall: 0.5831 - accuracy: 0.5412 - val_loss: 0.6914 - val_precision: 0.5392 - val_recall: 0.5500 - val_accuracy: 0.5400\n",
      "Epoch 141/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6909 - precision: 0.5398 - recall: 0.5946 - accuracy: 0.5438 - val_loss: 0.6855 - val_precision: 0.5810 - val_recall: 0.6100 - val_accuracy: 0.5850\n",
      "Epoch 142/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6847 - precision: 0.5784 - recall: 0.5760 - accuracy: 0.5778 - val_loss: 0.6864 - val_precision: 0.5794 - val_recall: 0.6200 - val_accuracy: 0.5850\n",
      "Epoch 143/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6889 - precision: 0.5392 - recall: 0.5467 - accuracy: 0.5392 - val_loss: 0.6872 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 144/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6825 - precision: 0.5880 - recall: 0.6401 - accuracy: 0.5960 - val_loss: 0.6887 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 145/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6884 - precision: 0.5495 - recall: 0.6055 - accuracy: 0.5545 - val_loss: 0.6870 - val_precision: 0.5741 - val_recall: 0.6200 - val_accuracy: 0.5800\n",
      "Epoch 146/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6858 - precision: 0.5594 - recall: 0.6161 - accuracy: 0.5654 - val_loss: 0.6872 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 147/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6840 - precision: 0.5734 - recall: 0.5919 - accuracy: 0.5756 - val_loss: 0.6874 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 148/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6898 - precision: 0.5311 - recall: 0.5732 - accuracy: 0.5343 - val_loss: 0.6874 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 149/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6821 - precision: 0.5870 - recall: 0.5838 - accuracy: 0.5860 - val_loss: 0.6861 - val_precision: 0.5701 - val_recall: 0.6100 - val_accuracy: 0.5750\n",
      "Epoch 150/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6859 - precision: 0.5537 - recall: 0.5906 - accuracy: 0.5574 - val_loss: 0.6858 - val_precision: 0.5755 - val_recall: 0.6100 - val_accuracy: 0.5800\n",
      "Epoch 151/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6836 - precision: 0.5743 - recall: 0.5929 - accuracy: 0.5768 - val_loss: 0.6905 - val_precision: 0.5545 - val_recall: 0.5600 - val_accuracy: 0.5550\n",
      "Epoch 152/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6848 - precision: 0.5579 - recall: 0.6160 - accuracy: 0.5638 - val_loss: 0.6903 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 153/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6883 - precision: 0.5382 - recall: 0.5810 - accuracy: 0.5413 - val_loss: 0.6870 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 154/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6846 - precision: 0.5678 - recall: 0.5735 - accuracy: 0.5691 - val_loss: 0.6894 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 155/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6847 - precision: 0.5606 - recall: 0.6452 - accuracy: 0.5701 - val_loss: 0.6926 - val_precision: 0.5098 - val_recall: 0.5200 - val_accuracy: 0.5100\n",
      "Epoch 156/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6889 - precision: 0.5397 - recall: 0.5744 - accuracy: 0.5418 - val_loss: 0.6893 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 157/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6835 - precision: 0.5930 - recall: 0.5719 - accuracy: 0.5894 - val_loss: 0.6901 - val_precision: 0.5545 - val_recall: 0.5600 - val_accuracy: 0.5550\n",
      "Epoch 158/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6824 - precision: 0.5850 - recall: 0.6080 - accuracy: 0.5881 - val_loss: 0.6855 - val_precision: 0.5794 - val_recall: 0.6200 - val_accuracy: 0.5850\n",
      "Epoch 159/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6827 - precision: 0.5832 - recall: 0.6090 - accuracy: 0.5869 - val_loss: 0.6886 - val_precision: 0.5377 - val_recall: 0.5700 - val_accuracy: 0.5400\n",
      "Epoch 160/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6859 - precision: 0.5654 - recall: 0.6013 - accuracy: 0.5701 - val_loss: 0.6917 - val_precision: 0.5437 - val_recall: 0.5600 - val_accuracy: 0.5450\n",
      "Epoch 161/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6908 - precision: 0.5331 - recall: 0.5572 - accuracy: 0.5368 - val_loss: 0.6878 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 162/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6846 - precision: 0.5768 - recall: 0.6124 - accuracy: 0.5809 - val_loss: 0.6892 - val_precision: 0.5385 - val_recall: 0.5600 - val_accuracy: 0.5400\n",
      "Epoch 163/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6832 - precision: 0.5675 - recall: 0.5877 - accuracy: 0.5696 - val_loss: 0.6910 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 164/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6874 - precision: 0.5536 - recall: 0.5801 - accuracy: 0.5563 - val_loss: 0.6907 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 165/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6830 - precision: 0.5892 - recall: 0.6617 - accuracy: 0.6002 - val_loss: 0.6901 - val_precision: 0.5437 - val_recall: 0.5600 - val_accuracy: 0.5450\n",
      "Epoch 166/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6858 - precision: 0.5660 - recall: 0.6582 - accuracy: 0.5750 - val_loss: 0.6890 - val_precision: 0.5429 - val_recall: 0.5700 - val_accuracy: 0.5450\n",
      "Epoch 167/280\n",
      "128/128 [==============================] - 86s 672ms/step - loss: 0.6842 - precision: 0.5799 - recall: 0.6015 - accuracy: 0.5828 - val_loss: 0.6867 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 168/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6846 - precision: 0.5665 - recall: 0.5923 - accuracy: 0.5697 - val_loss: 0.6886 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 169/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6892 - precision: 0.5466 - recall: 0.5707 - accuracy: 0.5494 - val_loss: 0.6866 - val_precision: 0.5741 - val_recall: 0.6200 - val_accuracy: 0.5800\n",
      "Epoch 170/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6846 - precision: 0.5706 - recall: 0.6084 - accuracy: 0.5731 - val_loss: 0.6871 - val_precision: 0.5673 - val_recall: 0.5900 - val_accuracy: 0.5700\n",
      "Epoch 171/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6879 - precision: 0.5512 - recall: 0.5844 - accuracy: 0.5542 - val_loss: 0.6851 - val_precision: 0.5794 - val_recall: 0.6200 - val_accuracy: 0.5850\n",
      "Epoch 172/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6830 - precision: 0.6025 - recall: 0.6089 - accuracy: 0.6031 - val_loss: 0.6852 - val_precision: 0.5727 - val_recall: 0.6300 - val_accuracy: 0.5800\n",
      "Epoch 173/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6836 - precision: 0.5824 - recall: 0.6123 - accuracy: 0.5859 - val_loss: 0.6845 - val_precision: 0.5833 - val_recall: 0.6300 - val_accuracy: 0.5900\n",
      "Epoch 174/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6799 - precision: 0.5990 - recall: 0.6354 - accuracy: 0.6040 - val_loss: 0.6871 - val_precision: 0.5631 - val_recall: 0.5800 - val_accuracy: 0.5650\n",
      "Epoch 175/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6877 - precision: 0.5502 - recall: 0.5626 - accuracy: 0.5514 - val_loss: 0.6882 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 176/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6879 - precision: 0.5449 - recall: 0.5648 - accuracy: 0.5410 - val_loss: 0.6859 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 177/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6869 - precision: 0.5480 - recall: 0.5719 - accuracy: 0.5503 - val_loss: 0.6906 - val_precision: 0.5294 - val_recall: 0.5400 - val_accuracy: 0.5300\n",
      "Epoch 178/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6832 - precision: 0.5894 - recall: 0.6184 - accuracy: 0.5938 - val_loss: 0.6887 - val_precision: 0.5472 - val_recall: 0.5800 - val_accuracy: 0.5500\n",
      "Epoch 179/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6823 - precision: 0.5942 - recall: 0.6401 - accuracy: 0.6015 - val_loss: 0.6869 - val_precision: 0.5714 - val_recall: 0.6000 - val_accuracy: 0.5750\n",
      "Epoch 180/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6862 - precision: 0.5613 - recall: 0.5633 - accuracy: 0.5610 - val_loss: 0.6860 - val_precision: 0.5849 - val_recall: 0.6200 - val_accuracy: 0.5900\n",
      "Epoch 181/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6871 - precision: 0.5460 - recall: 0.5767 - accuracy: 0.5491 - val_loss: 0.6910 - val_precision: 0.5392 - val_recall: 0.5500 - val_accuracy: 0.5400\n",
      "Epoch 182/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6825 - precision: 0.5942 - recall: 0.6398 - accuracy: 0.6014 - val_loss: 0.6852 - val_precision: 0.5780 - val_recall: 0.6300 - val_accuracy: 0.5850\n",
      "Epoch 183/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6889 - precision: 0.5348 - recall: 0.5457 - accuracy: 0.5354 - val_loss: 0.6854 - val_precision: 0.5780 - val_recall: 0.6300 - val_accuracy: 0.5850\n",
      "Epoch 184/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6867 - precision: 0.5657 - recall: 0.6263 - accuracy: 0.5732 - val_loss: 0.6867 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 185/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6852 - precision: 0.5668 - recall: 0.5592 - accuracy: 0.5658 - val_loss: 0.6901 - val_precision: 0.5340 - val_recall: 0.5500 - val_accuracy: 0.5350\n",
      "Epoch 186/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6838 - precision: 0.5794 - recall: 0.6128 - accuracy: 0.5836 - val_loss: 0.6877 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 187/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6850 - precision: 0.5761 - recall: 0.5899 - accuracy: 0.5770 - val_loss: 0.6911 - val_precision: 0.5455 - val_recall: 0.5400 - val_accuracy: 0.5450\n",
      "Epoch 188/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6813 - precision: 0.6136 - recall: 0.6215 - accuracy: 0.6150 - val_loss: 0.6871 - val_precision: 0.5556 - val_recall: 0.6000 - val_accuracy: 0.5600\n",
      "Epoch 189/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6853 - precision: 0.5642 - recall: 0.5934 - accuracy: 0.5670 - val_loss: 0.6915 - val_precision: 0.5340 - val_recall: 0.5500 - val_accuracy: 0.5350\n",
      "Epoch 190/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6832 - precision: 0.5841 - recall: 0.6137 - accuracy: 0.5869 - val_loss: 0.6878 - val_precision: 0.5514 - val_recall: 0.5900 - val_accuracy: 0.5550\n",
      "Epoch 191/280\n",
      "128/128 [==============================] - 86s 672ms/step - loss: 0.6882 - precision: 0.5571 - recall: 0.5631 - accuracy: 0.5570 - val_loss: 0.6907 - val_precision: 0.5392 - val_recall: 0.5500 - val_accuracy: 0.5400\n",
      "Epoch 192/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6844 - precision: 0.5646 - recall: 0.6331 - accuracy: 0.5719 - val_loss: 0.6894 - val_precision: 0.5481 - val_recall: 0.5700 - val_accuracy: 0.5500\n",
      "Epoch 193/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6899 - precision: 0.5402 - recall: 0.5731 - accuracy: 0.5422 - val_loss: 0.6866 - val_precision: 0.5741 - val_recall: 0.6200 - val_accuracy: 0.5800\n",
      "Epoch 194/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6901 - precision: 0.5395 - recall: 0.5600 - accuracy: 0.5406 - val_loss: 0.6875 - val_precision: 0.5534 - val_recall: 0.5700 - val_accuracy: 0.5550\n",
      "Epoch 195/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6858 - precision: 0.5574 - recall: 0.6251 - accuracy: 0.5651 - val_loss: 0.6905 - val_precision: 0.5392 - val_recall: 0.5500 - val_accuracy: 0.5400\n",
      "Epoch 196/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6835 - precision: 0.5893 - recall: 0.6237 - accuracy: 0.5949 - val_loss: 0.6871 - val_precision: 0.5673 - val_recall: 0.5900 - val_accuracy: 0.5700\n",
      "Epoch 197/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6797 - precision: 0.6042 - recall: 0.6449 - accuracy: 0.6114 - val_loss: 0.6872 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 198/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6835 - precision: 0.5876 - recall: 0.5968 - accuracy: 0.5889 - val_loss: 0.6910 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 199/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6824 - precision: 0.5791 - recall: 0.6212 - accuracy: 0.5847 - val_loss: 0.6879 - val_precision: 0.5631 - val_recall: 0.5800 - val_accuracy: 0.5650\n",
      "Epoch 200/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6872 - precision: 0.5586 - recall: 0.5577 - accuracy: 0.5592 - val_loss: 0.6855 - val_precision: 0.5741 - val_recall: 0.6200 - val_accuracy: 0.5800\n",
      "Epoch 201/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6827 - precision: 0.5884 - recall: 0.5880 - accuracy: 0.5862 - val_loss: 0.6870 - val_precision: 0.5619 - val_recall: 0.5900 - val_accuracy: 0.5650\n",
      "Epoch 202/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6910 - precision: 0.5243 - recall: 0.5569 - accuracy: 0.5312 - val_loss: 0.6890 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 203/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6834 - precision: 0.5818 - recall: 0.6148 - accuracy: 0.5865 - val_loss: 0.6890 - val_precision: 0.5437 - val_recall: 0.5600 - val_accuracy: 0.5450\n",
      "Epoch 204/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6854 - precision: 0.5698 - recall: 0.5589 - accuracy: 0.5669 - val_loss: 0.6869 - val_precision: 0.5673 - val_recall: 0.5900 - val_accuracy: 0.5700\n",
      "Epoch 205/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6848 - precision: 0.5727 - recall: 0.6338 - accuracy: 0.5801 - val_loss: 0.6889 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 206/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6874 - precision: 0.5648 - recall: 0.5477 - accuracy: 0.5624 - val_loss: 0.6881 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 207/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6892 - precision: 0.5379 - recall: 0.5877 - accuracy: 0.5418 - val_loss: 0.6868 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 208/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6835 - precision: 0.5943 - recall: 0.6088 - accuracy: 0.5964 - val_loss: 0.6883 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 209/280\n",
      "128/128 [==============================] - 87s 677ms/step - loss: 0.6844 - precision: 0.5638 - recall: 0.6580 - accuracy: 0.5744 - val_loss: 0.6904 - val_precision: 0.5455 - val_recall: 0.5400 - val_accuracy: 0.5450\n",
      "Epoch 210/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6836 - precision: 0.5832 - recall: 0.5926 - accuracy: 0.5841 - val_loss: 0.6905 - val_precision: 0.5392 - val_recall: 0.5500 - val_accuracy: 0.5400\n",
      "Epoch 211/280\n",
      "128/128 [==============================] - 86s 672ms/step - loss: 0.6883 - precision: 0.5539 - recall: 0.5793 - accuracy: 0.5560 - val_loss: 0.6859 - val_precision: 0.5688 - val_recall: 0.6200 - val_accuracy: 0.5750\n",
      "Epoch 212/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6839 - precision: 0.5782 - recall: 0.6011 - accuracy: 0.5818 - val_loss: 0.6895 - val_precision: 0.5437 - val_recall: 0.5600 - val_accuracy: 0.5450\n",
      "Epoch 213/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6854 - precision: 0.5844 - recall: 0.6001 - accuracy: 0.5861 - val_loss: 0.6862 - val_precision: 0.5865 - val_recall: 0.6100 - val_accuracy: 0.5900\n",
      "Epoch 214/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6855 - precision: 0.5725 - recall: 0.5713 - accuracy: 0.5717 - val_loss: 0.6925 - val_precision: 0.5288 - val_recall: 0.5500 - val_accuracy: 0.5300\n",
      "Epoch 215/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6817 - precision: 0.6102 - recall: 0.6105 - accuracy: 0.6094 - val_loss: 0.6875 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 216/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6890 - precision: 0.5446 - recall: 0.5632 - accuracy: 0.5461 - val_loss: 0.6843 - val_precision: 0.5818 - val_recall: 0.6400 - val_accuracy: 0.5900\n",
      "Epoch 217/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6871 - precision: 0.5527 - recall: 0.6139 - accuracy: 0.5573 - val_loss: 0.6900 - val_precision: 0.5385 - val_recall: 0.5600 - val_accuracy: 0.5400\n",
      "Epoch 218/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6843 - precision: 0.5704 - recall: 0.5877 - accuracy: 0.5731 - val_loss: 0.6863 - val_precision: 0.5701 - val_recall: 0.6100 - val_accuracy: 0.5750\n",
      "Epoch 219/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6870 - precision: 0.5580 - recall: 0.5591 - accuracy: 0.5579 - val_loss: 0.6855 - val_precision: 0.5766 - val_recall: 0.6400 - val_accuracy: 0.5850\n",
      "Epoch 220/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6907 - precision: 0.5292 - recall: 0.5697 - accuracy: 0.5314 - val_loss: 0.6841 - val_precision: 0.5926 - val_recall: 0.6400 - val_accuracy: 0.6000\n",
      "Epoch 221/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6863 - precision: 0.5641 - recall: 0.5740 - accuracy: 0.5653 - val_loss: 0.6897 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 222/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6881 - precision: 0.5486 - recall: 0.5466 - accuracy: 0.5485 - val_loss: 0.6856 - val_precision: 0.5833 - val_recall: 0.6300 - val_accuracy: 0.5900\n",
      "Epoch 223/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6857 - precision: 0.5671 - recall: 0.5868 - accuracy: 0.5691 - val_loss: 0.6857 - val_precision: 0.5794 - val_recall: 0.6200 - val_accuracy: 0.5850\n",
      "Epoch 224/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6867 - precision: 0.5572 - recall: 0.6008 - accuracy: 0.5618 - val_loss: 0.6881 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 225/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6873 - precision: 0.5655 - recall: 0.5667 - accuracy: 0.5660 - val_loss: 0.6860 - val_precision: 0.5741 - val_recall: 0.6200 - val_accuracy: 0.5800\n",
      "Epoch 226/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6848 - precision: 0.5756 - recall: 0.6232 - accuracy: 0.5816 - val_loss: 0.6888 - val_precision: 0.5437 - val_recall: 0.5600 - val_accuracy: 0.5450\n",
      "Epoch 227/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6872 - precision: 0.5548 - recall: 0.5684 - accuracy: 0.5571 - val_loss: 0.6905 - val_precision: 0.5392 - val_recall: 0.5500 - val_accuracy: 0.5400\n",
      "Epoch 228/280\n",
      "128/128 [==============================] - 86s 671ms/step - loss: 0.6820 - precision: 0.5976 - recall: 0.6149 - accuracy: 0.5976 - val_loss: 0.6881 - val_precision: 0.5514 - val_recall: 0.5900 - val_accuracy: 0.5550\n",
      "Epoch 229/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6860 - precision: 0.5721 - recall: 0.5872 - accuracy: 0.5738 - val_loss: 0.6860 - val_precision: 0.5769 - val_recall: 0.6000 - val_accuracy: 0.5800\n",
      "Epoch 230/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6880 - precision: 0.5391 - recall: 0.6020 - accuracy: 0.5435 - val_loss: 0.6867 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 231/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6894 - precision: 0.5361 - recall: 0.5835 - accuracy: 0.5386 - val_loss: 0.6894 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 232/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6883 - precision: 0.5443 - recall: 0.5637 - accuracy: 0.5455 - val_loss: 0.6905 - val_precision: 0.5545 - val_recall: 0.5600 - val_accuracy: 0.5550\n",
      "Epoch 233/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6847 - precision: 0.5719 - recall: 0.5628 - accuracy: 0.5676 - val_loss: 0.6902 - val_precision: 0.5347 - val_recall: 0.5400 - val_accuracy: 0.5350\n",
      "Epoch 234/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6860 - precision: 0.5616 - recall: 0.5457 - accuracy: 0.5601 - val_loss: 0.6878 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 235/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6863 - precision: 0.5587 - recall: 0.6150 - accuracy: 0.5642 - val_loss: 0.6909 - val_precision: 0.5400 - val_recall: 0.5400 - val_accuracy: 0.5400\n",
      "Epoch 236/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6842 - precision: 0.5639 - recall: 0.5913 - accuracy: 0.5652 - val_loss: 0.6902 - val_precision: 0.5392 - val_recall: 0.5500 - val_accuracy: 0.5400\n",
      "Epoch 237/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6839 - precision: 0.5793 - recall: 0.6099 - accuracy: 0.5834 - val_loss: 0.6895 - val_precision: 0.5429 - val_recall: 0.5700 - val_accuracy: 0.5450\n",
      "Epoch 238/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6891 - precision: 0.5377 - recall: 0.6075 - accuracy: 0.5424 - val_loss: 0.6893 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 239/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6836 - precision: 0.5833 - recall: 0.5852 - accuracy: 0.5822 - val_loss: 0.6896 - val_precision: 0.5481 - val_recall: 0.5700 - val_accuracy: 0.5500\n",
      "Epoch 240/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6883 - precision: 0.5419 - recall: 0.5846 - accuracy: 0.5452 - val_loss: 0.6855 - val_precision: 0.5833 - val_recall: 0.6300 - val_accuracy: 0.5900\n",
      "Epoch 241/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6882 - precision: 0.5461 - recall: 0.6014 - accuracy: 0.5512 - val_loss: 0.6884 - val_precision: 0.5534 - val_recall: 0.5700 - val_accuracy: 0.5550\n",
      "Epoch 242/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6872 - precision: 0.5611 - recall: 0.5947 - accuracy: 0.5648 - val_loss: 0.6850 - val_precision: 0.5833 - val_recall: 0.6300 - val_accuracy: 0.5900\n",
      "Epoch 243/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6821 - precision: 0.5824 - recall: 0.6217 - accuracy: 0.5880 - val_loss: 0.6892 - val_precision: 0.5534 - val_recall: 0.5700 - val_accuracy: 0.5550\n",
      "Epoch 244/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6890 - precision: 0.5415 - recall: 0.5392 - accuracy: 0.5410 - val_loss: 0.6866 - val_precision: 0.5728 - val_recall: 0.5900 - val_accuracy: 0.5750\n",
      "Epoch 245/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6844 - precision: 0.5785 - recall: 0.5899 - accuracy: 0.5839 - val_loss: 0.6875 - val_precision: 0.5524 - val_recall: 0.5800 - val_accuracy: 0.5550\n",
      "Epoch 246/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6824 - precision: 0.5825 - recall: 0.5983 - accuracy: 0.5836 - val_loss: 0.6878 - val_precision: 0.5514 - val_recall: 0.5900 - val_accuracy: 0.5550\n",
      "Epoch 247/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6856 - precision: 0.5516 - recall: 0.6084 - accuracy: 0.5564 - val_loss: 0.6907 - val_precision: 0.5340 - val_recall: 0.5500 - val_accuracy: 0.5350\n",
      "Epoch 248/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6839 - precision: 0.5889 - recall: 0.5948 - accuracy: 0.5896 - val_loss: 0.6916 - val_precision: 0.5294 - val_recall: 0.5400 - val_accuracy: 0.5300\n",
      "Epoch 249/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6915 - precision: 0.5149 - recall: 0.5386 - accuracy: 0.5147 - val_loss: 0.6904 - val_precision: 0.5500 - val_recall: 0.5500 - val_accuracy: 0.5500\n",
      "Epoch 250/280\n",
      "128/128 [==============================] - 86s 672ms/step - loss: 0.6864 - precision: 0.5613 - recall: 0.6133 - accuracy: 0.5675 - val_loss: 0.6875 - val_precision: 0.5514 - val_recall: 0.5900 - val_accuracy: 0.5550\n",
      "Epoch 251/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6860 - precision: 0.5755 - recall: 0.5826 - accuracy: 0.5755 - val_loss: 0.6892 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 252/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6869 - precision: 0.5589 - recall: 0.6064 - accuracy: 0.5635 - val_loss: 0.6906 - val_precision: 0.5385 - val_recall: 0.5600 - val_accuracy: 0.5400\n",
      "Epoch 253/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6837 - precision: 0.5830 - recall: 0.6084 - accuracy: 0.5862 - val_loss: 0.6873 - val_precision: 0.5607 - val_recall: 0.6000 - val_accuracy: 0.5650\n",
      "Epoch 254/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6817 - precision: 0.5934 - recall: 0.6195 - accuracy: 0.5973 - val_loss: 0.6865 - val_precision: 0.5556 - val_recall: 0.6000 - val_accuracy: 0.5600\n",
      "Epoch 255/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6862 - precision: 0.5576 - recall: 0.6012 - accuracy: 0.5615 - val_loss: 0.6858 - val_precision: 0.5849 - val_recall: 0.6200 - val_accuracy: 0.5900\n",
      "Epoch 256/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6824 - precision: 0.5853 - recall: 0.5839 - accuracy: 0.5852 - val_loss: 0.6866 - val_precision: 0.5741 - val_recall: 0.6200 - val_accuracy: 0.5800\n",
      "Epoch 257/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6865 - precision: 0.5460 - recall: 0.5634 - accuracy: 0.5481 - val_loss: 0.6912 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 258/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6874 - precision: 0.5521 - recall: 0.5808 - accuracy: 0.5525 - val_loss: 0.6907 - val_precision: 0.5347 - val_recall: 0.5400 - val_accuracy: 0.5350\n",
      "Epoch 259/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6872 - precision: 0.5642 - recall: 0.5950 - accuracy: 0.5686 - val_loss: 0.6892 - val_precision: 0.5429 - val_recall: 0.5700 - val_accuracy: 0.5450\n",
      "Epoch 260/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6847 - precision: 0.5778 - recall: 0.6422 - accuracy: 0.5868 - val_loss: 0.6864 - val_precision: 0.5648 - val_recall: 0.6100 - val_accuracy: 0.5700\n",
      "Epoch 261/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6860 - precision: 0.5700 - recall: 0.5915 - accuracy: 0.5729 - val_loss: 0.6853 - val_precision: 0.5648 - val_recall: 0.6100 - val_accuracy: 0.5700\n",
      "Epoch 262/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6898 - precision: 0.5323 - recall: 0.5669 - accuracy: 0.5336 - val_loss: 0.6854 - val_precision: 0.5849 - val_recall: 0.6200 - val_accuracy: 0.5900\n",
      "Epoch 263/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6901 - precision: 0.5346 - recall: 0.5999 - accuracy: 0.5380 - val_loss: 0.6859 - val_precision: 0.5780 - val_recall: 0.6300 - val_accuracy: 0.5850\n",
      "Epoch 264/280\n",
      "128/128 [==============================] - 86s 673ms/step - loss: 0.6861 - precision: 0.5625 - recall: 0.6147 - accuracy: 0.5686 - val_loss: 0.6917 - val_precision: 0.5294 - val_recall: 0.5400 - val_accuracy: 0.5300\n",
      "Epoch 265/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6850 - precision: 0.5854 - recall: 0.6142 - accuracy: 0.5900 - val_loss: 0.6863 - val_precision: 0.5741 - val_recall: 0.6200 - val_accuracy: 0.5800\n",
      "Epoch 266/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6845 - precision: 0.5744 - recall: 0.5908 - accuracy: 0.5769 - val_loss: 0.6885 - val_precision: 0.5514 - val_recall: 0.5900 - val_accuracy: 0.5550\n",
      "Epoch 267/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6830 - precision: 0.5843 - recall: 0.6054 - accuracy: 0.5868 - val_loss: 0.6860 - val_precision: 0.5794 - val_recall: 0.6200 - val_accuracy: 0.5850\n",
      "Epoch 268/280\n",
      "128/128 [==============================] - 87s 676ms/step - loss: 0.6822 - precision: 0.5864 - recall: 0.5928 - accuracy: 0.5869 - val_loss: 0.6871 - val_precision: 0.5660 - val_recall: 0.6000 - val_accuracy: 0.5700\n",
      "Epoch 269/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6900 - precision: 0.5282 - recall: 0.5655 - accuracy: 0.5293 - val_loss: 0.6923 - val_precision: 0.5385 - val_recall: 0.5600 - val_accuracy: 0.5400\n",
      "Epoch 270/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6830 - precision: 0.5814 - recall: 0.6459 - accuracy: 0.5893 - val_loss: 0.6877 - val_precision: 0.5566 - val_recall: 0.5900 - val_accuracy: 0.5600\n",
      "Epoch 271/280\n",
      "128/128 [==============================] - 86s 671ms/step - loss: 0.6881 - precision: 0.5462 - recall: 0.5880 - accuracy: 0.5495 - val_loss: 0.6848 - val_precision: 0.5833 - val_recall: 0.6300 - val_accuracy: 0.5900\n",
      "Epoch 272/280\n",
      "128/128 [==============================] - 86s 672ms/step - loss: 0.6889 - precision: 0.5394 - recall: 0.5446 - accuracy: 0.5402 - val_loss: 0.6864 - val_precision: 0.5714 - val_recall: 0.6000 - val_accuracy: 0.5750\n",
      "Epoch 273/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6897 - precision: 0.5276 - recall: 0.5335 - accuracy: 0.5283 - val_loss: 0.6883 - val_precision: 0.5534 - val_recall: 0.5700 - val_accuracy: 0.5550\n",
      "Epoch 274/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6888 - precision: 0.5521 - recall: 0.5920 - accuracy: 0.5553 - val_loss: 0.6913 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n",
      "Epoch 275/280\n",
      "128/128 [==============================] - 86s 675ms/step - loss: 0.6890 - precision: 0.5413 - recall: 0.5512 - accuracy: 0.5416 - val_loss: 0.6874 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 276/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6849 - precision: 0.5672 - recall: 0.6084 - accuracy: 0.5718 - val_loss: 0.6879 - val_precision: 0.5577 - val_recall: 0.5800 - val_accuracy: 0.5600\n",
      "Epoch 277/280\n",
      "128/128 [==============================] - 87s 678ms/step - loss: 0.6862 - precision: 0.5635 - recall: 0.5662 - accuracy: 0.5640 - val_loss: 0.6907 - val_precision: 0.5490 - val_recall: 0.5600 - val_accuracy: 0.5500\n",
      "Epoch 278/280\n",
      "128/128 [==============================] - 86s 674ms/step - loss: 0.6863 - precision: 0.5746 - recall: 0.5806 - accuracy: 0.5720 - val_loss: 0.6920 - val_precision: 0.5294 - val_recall: 0.5400 - val_accuracy: 0.5300\n",
      "Epoch 279/280\n",
      "128/128 [==============================] - 86s 676ms/step - loss: 0.6862 - precision: 0.5552 - recall: 0.6189 - accuracy: 0.5618 - val_loss: 0.6902 - val_precision: 0.5534 - val_recall: 0.5700 - val_accuracy: 0.5550\n",
      "Epoch 280/280\n",
      "128/128 [==============================] - 86s 672ms/step - loss: 0.6842 - precision: 0.5663 - recall: 0.5888 - accuracy: 0.5692 - val_loss: 0.6909 - val_precision: 0.5446 - val_recall: 0.5500 - val_accuracy: 0.5450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feff74ad2e8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(generate(info_data, batch_size), \n",
    "          epochs = epochs, \n",
    "          steps_per_epoch= steps_epoch,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          validation_data=val_set,\n",
    "          validation_batch_size=8,\n",
    "          shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"enet-02.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"test06-interrupt.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation:\n",
    "Simple, just feed pairs from the validation split and see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = get_batch(info_data, 200, False, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(val_set_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = [y == (p>0) for y, p in zip (val_set_y, preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(preds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = np.zeros((2,2))\n",
    "for y,p in zip (val_set_y, preds):\n",
    "    cs[int(y), round(p[0])] += 1 \n",
    "acc = (cs[0,0] + cs [1 , 1]) / np.sum(cs)\n",
    "rec = cs[1,1] / np.sum(cs[1])\n",
    "print(cs, acc, rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended validation:\n",
    "Take two images, and compare strip by strip, then do a majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_batch(info_data, num_whole_images, batch_size):\n",
    "    images = []\n",
    "    split_index = int((len(info_data)-1) * 0.8)\n",
    "    students = [rng.randint(split_index+1, len(info_data)-1) for _ in range(num_whole_images)] \n",
    "    labels = np.zeros((num_whole_images, ))\n",
    "    labels[num_whole_images//2:] = 1\n",
    "    id_pairs_img = []\n",
    "    for j in range(num_whole_images):\n",
    "        pairs = [np.zeros((batch_size, STRIP_SIZE, STRIP_SIZE, 3)) for i in range(2)]\n",
    "        imgs = [rng.randint(0, len(info_data[i])-1) for i in students]\n",
    "        std = students[j]\n",
    "        img = imgs[j]\n",
    "        id_pairs = []\n",
    "        std2 = (std + rng.randint(1, len(info_data)-1)) % len(info_data)\n",
    "        if j >= num_whole_images // 2:   \n",
    "            img2 = (img + rng.randint(1, len(info_data[std])-1)) % len(info_data[std])\n",
    "            std2 = std\n",
    "        else:\n",
    "            std2 = (std + rng.randint(1, len(info_data)-1)) % len(info_data)\n",
    "            img2 = rng.randint(0, len(info_data[std2])-1)\n",
    "        if batch_size > len(info_data[std][img][1]):\n",
    "            print(\"a\", info_data[std][0][0])\n",
    "            std = std+1\n",
    "            \n",
    "        if batch_size > len(info_data[std2][img2][1]):\n",
    "            print(\"b\", info_data[std2][0][0])\n",
    "            std2 = std+1\n",
    "            \n",
    "        for i in range(batch_size):    \n",
    "            strip1 = info_data[std][img][1][i]\n",
    "            strip2 = info_data[std2][img2][1][i]\n",
    "#             print(strip1, strip2)\n",
    "            pairs[0][i,:,:,:] = cv2.imread(strip1)/255\n",
    "            pairs[1][i,:,:,:] = cv2.imread(strip2)/255\n",
    "            id_pairs.append((strip1, strip2))\n",
    "        images.append(pairs)\n",
    "        id_pairs_img.append(id_pairs)\n",
    "    \n",
    "    return images, labels, id_pairs_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_i, t_l, t_d = get_val_batch(info_data, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = np.zeros((2,2))\n",
    "cs_all = np.zeros((2,2))\n",
    "for i, l, d in zip(t_i, t_l, t_d):\n",
    "    preds = model.predict(i)\n",
    "    for p in preds:\n",
    "        cs_all[int(l), round(p[0])] += 1\n",
    "#     print(int(l), round(np.mean(preds)), (np.mean(preds)))\n",
    "    fp = sum([p[0] > 0.5 for p in preds]) > len(preds)//2 -2\n",
    "    cs[int(l), int(fp)] +=1\n",
    "#     print(l, d[0])\n",
    "\n",
    "acc = (cs[0,0] + cs [1 , 1]) / np.sum(cs)\n",
    "rec = cs[1,1] / np.sum(cs[1])\n",
    "print(cs, acc, rec)\n",
    "cs = cs_all\n",
    "acc = (cs[0,0] + cs [1 , 1]) / np.sum(cs)\n",
    "rec = cs[1,1] / np.sum(cs[1])\n",
    "print(cs, acc, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
