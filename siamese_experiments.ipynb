{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random as rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset structure\n",
    "\n",
    "The initial idea is to save the images whole, and let a get_batch function do the cutting and pairing when creating the batch. The strcuture of the dataset is going to be as follows:\n",
    "\n",
    "* images\n",
    "    * train\n",
    "        * s000 (infered by the names of the images)\n",
    "            * s000img1 (actually original name of the image)\n",
    "            * s000img2\n",
    "            * ...\n",
    "        * S001 \n",
    "            * etc.\n",
    "    * valid\n",
    "        * s700\n",
    "            * etc.\n",
    "    * test\n",
    "        * s999\n",
    "            * etc.\n",
    "\n",
    "For now, i will skip this structure creation and just assume it, for the experimenting enviroment, I will upload a few pictures and manually define this structure, and just work with train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIP_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now im storing the images whole. With the strips dictionary, given a strip we can get the image location and the pixels of the strip.\n",
    "However, in practice, this means that an image may be read multiple times when generating batches, also, it means that we read a whole big-ass image, while only needing a small part of it. A better approach would be to store the strips already as separate images beforehand, which unfortunately may increase the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(base_path = \"images\", s = \"train\", strips_device = 4, strip_size = 256):\n",
    "    devices = {}\n",
    "    strips = {}\n",
    "    s_path = os.path.join(base_path, s)\n",
    "    strip_num = 0\n",
    "    for device in os.scandir(s_path):\n",
    "        if len(device.name) != 3:\n",
    "            continue\n",
    "        d_num = int(device.name)\n",
    "        devices[d_num] = device.path\n",
    "#        imgs = os.listdir(device)\n",
    "        for strip in range(strips_device): #I'm only taking 4 strips for image. (top-left 4) and assuming I always can \n",
    "            x = strip * strip_size         #Better logic should be impl. to get the max. amount of strips per img size\n",
    "            y = 0                        \n",
    "            strips[strip_num] = (d_num, x, y)\n",
    "            strip_num += 1\n",
    "    return devices, strips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, s = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, devices, strips, imgids = False):\n",
    "    #choose at random some categories (category = strip location of a given device)\n",
    "    categories = rng.sample(range(0, len(strips)), k=batch_size)\n",
    "    \n",
    "    pairs = [np.zeros((batch_size, STRIP_SIZE, STRIP_SIZE, 3)) for i in range(2)]\n",
    "    labels = np.zeros((batch_size, ))\n",
    "    ids = []\n",
    "    \n",
    "    #the first half of the batch will be positive examples and the second negative\n",
    "    labels[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        cat = categories[i]\n",
    "        x = strips[cat][1] # x position of the strip\n",
    "        y = strips[cat][2] # y position of the strip\n",
    "        imgs_paths = [os.path.join(devices[strips[cat][0]], l) for l in os.listdir(devices[strips[cat][0]])]\n",
    "        if i >= batch_size // 2:\n",
    "            idxs = rng.sample(range(0, len(imgs_paths)-1), k=2)\n",
    "            img1 = cv2.imread(imgs_paths[idxs[0]])\n",
    "            img2 = cv2.imread(imgs_paths[idxs[1]])\n",
    "            pairs[0][i,:,:,:] = img1[x:x+STRIP_SIZE, y:y+STRIP_SIZE, :]\n",
    "            pairs[1][i,:,:,:] = img2[x:x+STRIP_SIZE, y:y+STRIP_SIZE, :]\n",
    "            ids.append((imgs_paths[idxs[0]], cat, imgs_paths[idxs[1]], cat))\n",
    "        else :\n",
    "            idx1 = rng.randint(0, len(imgs_paths)-1)\n",
    "            cat2 = (cat + rng.randint(1, len(categories))) % len(categories) # we ensure that is a different category\n",
    "            x2 = strips[cat2][1]\n",
    "            y2 = strips[cat2][2]            \n",
    "            imgs_paths2 = [os.path.join(devices[strips[cat2][0]], l) for l in os.listdir(devices[strips[cat2][0]])]\n",
    "            idx2 = rng.randint(0, len(imgs_paths2)-1)\n",
    "            img1 = cv2.imread(imgs_paths[idx1])\n",
    "            img2 = cv2.imread(imgs_paths2[idx2])\n",
    "            pairs[0][i,:,:,:] = img1[x:x+STRIP_SIZE, y:y+STRIP_SIZE, :]\n",
    "            pairs[1][i,:,:,:] = img2[x2:x2+STRIP_SIZE, y2:y2+STRIP_SIZE, :]\n",
    "            ids.append((imgs_paths[idx1], cat, imgs_paths[idx2], cat2))\n",
    "            \n",
    "    if imgids:\n",
    "        return pairs, labels, ids\n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset visualization\n",
    "\n",
    "We will now show some random examples to ensure that we are reading the dataset correctly. Each pair of images corresponds to a pair example, Pair0.0 are negative examples (belong to different strips of different images) and Pair1.0 are positive examples (belong to the same strip coordinates and the same devices (but different pictures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, labels, ids = get_batch(4,d ,s, True)\n",
    "for ix, i  in enumerate(ids):\n",
    "    f, ax = plt.subplots(1,2)\n",
    "    f.suptitle(\"Pair\" + str(labels[ix]) )\n",
    "    ax[0].imshow(pairs[0][ix,:,:,::-1]/255)\n",
    "    title = str(s[i[1]]) + \"\\n\" +  i[0].split('/')[-1] \n",
    "    ax[0].set_title(title)\n",
    "    ax[1].imshow(pairs[1][ix,:,:,::-1]/255)\n",
    "    title = str(s[i[3]]) + \"\\n\" +  i[2].split('/')[-1] \n",
    "    ax[1].set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition \n",
    "\n",
    "(ref: Siamese Neural Networks for One-shot Image Recognition, Koch et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, devices, strips, s=\"train\"):\n",
    "    while True:\n",
    "        pairs, labels = get_batch(batch_size, devices, strips, False)\n",
    "        yield(pairs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_block(input_shape = (256,256,3), base_filters=64):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(base_filters, (10,10), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(base_filters*2, (7,7), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(base_filters*2, (4,4), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(base_filters*4, (4,4), activation='relu'))\n",
    "#     model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(base_filters*32, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 247, 247, 64)      19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 123, 123, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 117, 117, 128)     401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 55, 55, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 27, 27, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 256)       524544    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24, 24, 2048)      526336    \n",
      "=================================================================\n",
      "Total params: 1,733,952\n",
      "Trainable params: 1,733,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = sequential_block(base_filters=64)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape = (256,256,3), base_filters=64):\n",
    "\n",
    "\n",
    "    left_input = layers.Input(input_shape)\n",
    "    right_input = layers.Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(base_filters, (10,10), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(base_filters*2, (7,7), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(base_filters*2, (4,4), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Conv2D(base_filters*4, (4,4), activation='relu'))\n",
    "#     model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(base_filters*64, activation='sigmoid'))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = layers.Dense(1,activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = keras.Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 24, 24, 4096) 2260288     input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 24, 24, 4096) 0           sequential_6[0][0]               \n",
      "                                                                 sequential_6[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 24, 24, 1)    4097        lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,264,385\n",
      "Trainable params: 2,264,385\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model(base_filters=64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
