{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese network implementation with the strips dataset\n",
    "\n",
    "last hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random as rng\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data = []\n",
    "with open('/home/data/strips_socrates/dataset_info.json') as json_file: \n",
    "    info_data = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIP_SIZE = 256\n",
    "SUB_STRIP_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(info_data, batch_size, examples, with_id=False, dataset = 'train'):\n",
    "    pairs = [np.zeros((batch_size, examples, STRIP_SIZE, STRIP_SIZE, 3)) for i in range(2)]\n",
    "    labels = np.zeros((batch_size, ))\n",
    "    labels[batch_size//2:] = 1\n",
    "    split_index = int((len(info_data)-1) * 0.8)\n",
    "    if dataset == 'train':\n",
    "        students = [rng.randint(0, split_index) for _ in range(batch_size)] \n",
    "    elif dataset == 'whole':\n",
    "        students = [rng.randint(0, (len(info_data)-1)) for _ in range(batch_size)] \n",
    "    else:\n",
    "        students = [rng.randint(split_index+1, len(info_data)-1) for _ in range(batch_size)] \n",
    "    imgs = [rng.randint(0, len(info_data[i])-1) for i in students]\n",
    "    strips_loc = [rng.randint(0, len(info_data[i][0][1])-1) for i in students]\n",
    "    id_pairs = []\n",
    "    for i in range(batch_size):\n",
    "        std = students[i]\n",
    "        img = imgs[i]\n",
    "        exl = [(img+j) % len(info_data[std]) for j in range(examples)]\n",
    "        srl = strips_loc[i]\n",
    "        strips1 = [ info_data[std][e][1][srl] for e in exl ]\n",
    "        strips2 = []\n",
    "        if i >= batch_size // 2:\n",
    "            img2 = (img + examples + rng.randint(1, len(info_data[std])-1)) % len(info_data[std])\n",
    "            exl2 = [(img2+j) % len(info_data[std]) for j in range(examples)]\n",
    "            strips2 = [ info_data[std][e][1][srl] for e in exl2]\n",
    "        else:\n",
    "            std2 = (std + rng.randint(1, len(info_data)-1)) % len(info_data)\n",
    "            img2 = rng.randint(0, len(info_data[std2])-1)\n",
    "            srl2 = rng.randint(0, len(info_data[std2][0][1])-1)\n",
    "            exl2 = [(img2+j) % len(info_data[std2]) for j in range(examples)]\n",
    "            strips2 = [ info_data[std2][e][1][srl2] for e in exl2]\n",
    "        for k in range(examples):\n",
    "            pairs[0][i,k,:,:,:] = cv2.imread(strips1[k])/255\n",
    "            pairs[1][i,k,:,:,:] = cv2.imread(strips2[k])/255\n",
    "        id_pairs.append((strips1, strips2))\n",
    "    if with_id:\n",
    "        return pairs, labels, id_pairs\n",
    "    else:\n",
    "        return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,l,ids = get_batch(info_data, 10, 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in zip(l, ids):\n",
    "    print(x[0])\n",
    "    for a in x[1]:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(info_data, examples, batch_size, dataset='train'):\n",
    "    while True:\n",
    "        pairs, labels = get_batch(info_data, batch_size, examples, False, dataset=dataset)\n",
    "        yield(pairs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_block(input_shape = (256,256,3), base_filters=64):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(base_filters, (3,3), input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (3,3)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (5,5)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (5,5)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (7,7)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(base_filters*64, activation='relu'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = sequential_block(base_filters=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 127, 127, 32)      224       \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 125, 125, 128)     36992     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 62, 62, 128)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 58, 58, 128)       409728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 29, 29, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 29, 29, 128)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 25, 25, 128)       409728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 12, 12, 128)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 6, 6, 128)         802944    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 3, 3, 128)         896       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2048)              2361344   \n",
      "=================================================================\n",
      "Total params: 4,025,440\n",
      "Trainable params: 4,022,720\n",
      "Non-trainable params: 2,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape = (4, 256,256,3), base_filters=16):\n",
    "\n",
    "    left_input = layers.Input(input_shape)\n",
    "    right_input = layers.Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    subshape = input_shape[1:4]\n",
    "    model = sequential_block(subshape, base_filters)\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encodeds_l = [model(left_input[:,k,...]) for k in range(input_shape[0])]\n",
    "    encodeds_r = [model(right_input[:,k,...]) for k in range(input_shape[0])]\n",
    "\n",
    "    encoded_l = layers.Average()(encodeds_l)\n",
    "    encoded_r = layers.Average()(encodeds_r)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = layers.Dense(1,activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = keras.Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                                      Output Shape                                Param #                 Connected to                                                      \n",
      "========================================================================================================================================================================================================\n",
      "input_1 (InputLayer)                                              [(None, 4, 256, 256, 3)]                    0                                                                                         \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "input_2 (InputLayer)                                              [(None, 4, 256, 256, 3)]                    0                                                                                         \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (SlicingOpLambda)                        (None, 256, 256, 3)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (SlicingOpLambda)                      (None, 256, 256, 3)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (SlicingOpLambda)                      (None, 256, 256, 3)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_3 (SlicingOpLambda)                      (None, 256, 256, 3)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_4 (SlicingOpLambda)                      (None, 256, 256, 3)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_5 (SlicingOpLambda)                      (None, 256, 256, 3)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_6 (SlicingOpLambda)                      (None, 256, 256, 3)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_7 (SlicingOpLambda)                      (None, 256, 256, 3)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)                                         (None, 2560)                                6287480                 tf.__operators__.getitem[0][0]                                    \n",
      "                                                                                                                                      tf.__operators__.getitem_1[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_2[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_3[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_4[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_5[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_6[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_7[0][0]                                  \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "average (Average)                                                 (None, 2560)                                0                       sequential_4[0][0]                                                \n",
      "                                                                                                                                      sequential_4[1][0]                                                \n",
      "                                                                                                                                      sequential_4[2][0]                                                \n",
      "                                                                                                                                      sequential_4[3][0]                                                \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "average_1 (Average)                                               (None, 2560)                                0                       sequential_4[4][0]                                                \n",
      "                                                                                                                                      sequential_4[5][0]                                                \n",
      "                                                                                                                                      sequential_4[6][0]                                                \n",
      "                                                                                                                                      sequential_4[7][0]                                                \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "lambda (Lambda)                                                   (None, 2560)                                0                       average[0][0]                                                     \n",
      "                                                                                                                                      average_1[0][0]                                                   \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "dense_5 (Dense)                                                   (None, 1)                                   2561                    lambda[0][0]                                                      \n",
      "========================================================================================================================================================================================================\n",
      "Total params: 6,290,041\n",
      "Trainable params: 6,286,641\n",
      "Non-trainable params: 3,400\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model(base_filters=40)\n",
    "model.summary(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-5\n",
    "end_learning_rate = 1e-7\n",
    "decay_steps = 12800\n",
    "lr_schedule = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate, decay_steps, end_learning_rate, power=0.5\n",
    ")\n",
    "model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        metrics=[keras.metrics.Precision(name='precision'),\n",
    "             keras.metrics.Recall(name='recall'), \"accuracy\"] \n",
    "    )\n",
    "run_name = \"last_hack01-renorm\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs/\"+run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 100\n",
    "examples = 4\n",
    "steps_epoch = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "128/128 [==============================] - 147s 1s/step - loss: 0.7415 - precision: 0.5349 - recall: 0.5947 - accuracy: 0.5347 - val_loss: 0.7066 - val_precision: 0.5200 - val_recall: 0.3900 - val_accuracy: 0.5150\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.7181 - precision: 0.5194 - recall: 0.3248 - accuracy: 0.5113 - val_loss: 0.7406 - val_precision: 0.5778 - val_recall: 0.2600 - val_accuracy: 0.5350\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6798 - precision: 0.5896 - recall: 0.3796 - accuracy: 0.5564 - val_loss: 0.7466 - val_precision: 0.5256 - val_recall: 0.4100 - val_accuracy: 0.5200\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6789 - precision: 0.5603 - recall: 0.3776 - accuracy: 0.5404 - val_loss: 0.7268 - val_precision: 0.6400 - val_recall: 0.3200 - val_accuracy: 0.5700\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6778 - precision: 0.5914 - recall: 0.3919 - accuracy: 0.5616 - val_loss: 0.7329 - val_precision: 0.6429 - val_recall: 0.2700 - val_accuracy: 0.5600\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6811 - precision: 0.5600 - recall: 0.4196 - accuracy: 0.5451 - val_loss: 0.7150 - val_precision: 0.5758 - val_recall: 0.3800 - val_accuracy: 0.5500\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 133s 1s/step - loss: 0.6731 - precision: 0.5925 - recall: 0.4572 - accuracy: 0.5710 - val_loss: 0.7743 - val_precision: 0.5410 - val_recall: 0.3300 - val_accuracy: 0.5250\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 134s 1s/step - loss: 0.6829 - precision: 0.5818 - recall: 0.4554 - accuracy: 0.5646 - val_loss: 0.7021 - val_precision: 0.6667 - val_recall: 0.3600 - val_accuracy: 0.5900\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 132s 1s/step - loss: 0.6783 - precision: 0.5565 - recall: 0.3654 - accuracy: 0.5386 - val_loss: 0.7300 - val_precision: 0.6364 - val_recall: 0.2100 - val_accuracy: 0.5450\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6900 - precision: 0.5872 - recall: 0.4172 - accuracy: 0.5631 - val_loss: 0.7425 - val_precision: 0.6154 - val_recall: 0.3200 - val_accuracy: 0.5600\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6613 - precision: 0.5957 - recall: 0.3317 - accuracy: 0.5542 - val_loss: 0.7310 - val_precision: 0.5067 - val_recall: 0.3800 - val_accuracy: 0.5050\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6922 - precision: 0.6311 - recall: 0.4065 - accuracy: 0.5846 - val_loss: 0.7007 - val_precision: 0.6481 - val_recall: 0.3500 - val_accuracy: 0.5800\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6781 - precision: 0.6034 - recall: 0.3762 - accuracy: 0.5638 - val_loss: 0.7718 - val_precision: 0.5893 - val_recall: 0.3300 - val_accuracy: 0.5500\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6700 - precision: 0.6403 - recall: 0.4212 - accuracy: 0.5927 - val_loss: 0.6805 - val_precision: 0.6905 - val_recall: 0.2900 - val_accuracy: 0.5800\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6800 - precision: 0.5753 - recall: 0.3552 - accuracy: 0.5462 - val_loss: 0.7169 - val_precision: 0.7027 - val_recall: 0.2600 - val_accuracy: 0.5750\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 130s 1s/step - loss: 0.6502 - precision: 0.6007 - recall: 0.3775 - accuracy: 0.5666 - val_loss: 0.6900 - val_precision: 0.6271 - val_recall: 0.3700 - val_accuracy: 0.5750\n",
      "Epoch 17/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6676 - precision: 0.5988 - recall: 0.4187 - accuracy: 0.5692 - val_loss: 0.6813 - val_precision: 0.5500 - val_recall: 0.4400 - val_accuracy: 0.5400\n",
      "Epoch 18/100\n",
      "128/128 [==============================] - 130s 1s/step - loss: 0.6561 - precision: 0.6233 - recall: 0.3935 - accuracy: 0.5765 - val_loss: 0.7011 - val_precision: 0.5593 - val_recall: 0.3300 - val_accuracy: 0.5350\n",
      "Epoch 19/100\n",
      "128/128 [==============================] - 130s 1s/step - loss: 0.6908 - precision: 0.5964 - recall: 0.4107 - accuracy: 0.5666 - val_loss: 0.7197 - val_precision: 0.5493 - val_recall: 0.3900 - val_accuracy: 0.5350\n",
      "Epoch 20/100\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6502 - precision: 0.6222 - recall: 0.4010 - accuracy: 0.5778 - val_loss: 0.6777 - val_precision: 0.6406 - val_recall: 0.4100 - val_accuracy: 0.5900\n",
      "Epoch 21/100\n",
      "128/128 [==============================] - 132s 1s/step - loss: 0.6857 - precision: 0.5818 - recall: 0.4539 - accuracy: 0.5638 - val_loss: 0.7198 - val_precision: 0.5957 - val_recall: 0.2800 - val_accuracy: 0.5450\n",
      "Epoch 22/100\n",
      " 64/128 [==============>...............] - ETA: 1:02 - loss: 0.6683 - precision: 0.5969 - recall: 0.3423 - accuracy: 0.5541"
     ]
    }
   ],
   "source": [
    "model.fit(generate(info_data, examples, batch_size), \n",
    "         epochs = epochs,\n",
    "         steps_per_epoch= steps_epoch,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          validation_batch_size=8,\n",
    "          validation_data=val_set,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"last-hack02.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"last-hack02.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation:\n",
    "Simple, just feed pairs from the validation split and see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9c8522421da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_batch' is not defined"
     ]
    }
   ],
   "source": [
    "val_set = get_batch(info_data, 500, 4, False, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(val_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = [y == (p>0) for y, p in zip (val_set_y, preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(preds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84. 16.]\n",
      " [74. 26.]] 0.55 0.26\n"
     ]
    }
   ],
   "source": [
    "cs = np.zeros((2,2))\n",
    "for y,p in zip (val_set[1], preds):\n",
    "    cs[int(y), round(p[0])] += 1 \n",
    "acc = (cs[0,0] + cs [1 , 1]) / np.sum(cs)\n",
    "rec = cs[1,1] / np.sum(cs[1])\n",
    "print(cs, acc, rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended validation:\n",
    "Take two images, and compare strip by strip, then do a majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_batch(info_data, num_whole_images, batch_size):\n",
    "    images = []\n",
    "    split_index = int((len(info_data)-1) * 0.8)\n",
    "    students = [rng.randint(split_index+1, len(info_data)-1) for _ in range(num_whole_images)] \n",
    "    labels = np.zeros((num_whole_images, ))\n",
    "    labels[num_whole_images//2:] = 1\n",
    "    id_pairs_img = []\n",
    "    for j in range(num_whole_images):\n",
    "        pairs = [np.zeros((batch_size, STRIP_SIZE, STRIP_SIZE, 3)) for i in range(2)]\n",
    "        imgs = [rng.randint(0, len(info_data[i])-1) for i in students]\n",
    "        std = students[j]\n",
    "        img = imgs[j]\n",
    "        id_pairs = []\n",
    "        std2 = (std + rng.randint(1, len(info_data)-1)) % len(info_data)\n",
    "        if j >= num_whole_images // 2:   \n",
    "            img2 = (img + rng.randint(1, len(info_data[std])-1)) % len(info_data[std])\n",
    "            std2 = std\n",
    "        else:\n",
    "            std2 = (std + rng.randint(1, len(info_data)-1)) % len(info_data)\n",
    "            img2 = rng.randint(0, len(info_data[std2])-1)\n",
    "        if batch_size > len(info_data[std][img][1]):\n",
    "            print(\"a\", info_data[std][0][0])\n",
    "            std = std+1\n",
    "            \n",
    "        if batch_size > len(info_data[std2][img2][1]):\n",
    "            print(\"b\", info_data[std2][0][0])\n",
    "            std2 = std+1\n",
    "            \n",
    "        for i in range(batch_size):    \n",
    "            strip1 = info_data[std][img][1][i]\n",
    "            strip2 = info_data[std2][img2][1][i]\n",
    "#             print(strip1, strip2)\n",
    "            pairs[0][i,:,:,:] = cv2.imread(strip1)/255\n",
    "            pairs[1][i,:,:,:] = cv2.imread(strip2)/255\n",
    "            id_pairs.append((strip1, strip2))\n",
    "        images.append(pairs)\n",
    "        id_pairs_img.append(id_pairs)\n",
    "    \n",
    "    return images, labels, id_pairs_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_i, t_l, t_d = get_val_batch(info_data, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = np.zeros((2,2))\n",
    "cs_all = np.zeros((2,2))\n",
    "for i, l, d in zip(t_i, t_l, t_d):\n",
    "    preds = model.predict(i)\n",
    "    for p in preds:\n",
    "        cs_all[int(l), round(p[0])] += 1\n",
    "#     print(int(l), round(np.mean(preds)), (np.mean(preds)))\n",
    "    fp = sum([p[0] > 0.5 for p in preds]) > len(preds)//2 \n",
    "    cs[int(l), int(fp)] +=1\n",
    "#     print(l, d[0])\n",
    "\n",
    "acc = (cs[0,0] + cs [1 , 1]) / np.sum(cs)\n",
    "rec = cs[1,1] / np.sum(cs[1])\n",
    "print(cs, acc, rec)\n",
    "cs = cs_all\n",
    "acc = (cs[0,0] + cs [1 , 1]) / np.sum(cs)\n",
    "rec = cs[1,1] / np.sum(cs[1])\n",
    "print(cs, acc, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
