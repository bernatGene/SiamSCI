{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese network implementation with the strips dataset\n",
    "\n",
    "With denoised and averaged images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structure\n",
    "\n",
    "The images themselves are stored already cut as strips, with the characteristic extracted by denoising them and getting the residual, as described in Lukas et al. We have an indexing file to retrieve them (info data) in the following structure:\n",
    "\n",
    "* Info data:\n",
    "    * Student 0:\n",
    "        * Image 0:\n",
    "            * Strip 0,0\n",
    "            * Strip 0,256\n",
    "            * Strip 0,512\n",
    "            * ...\n",
    "        * Image 1:\n",
    "            * Strip 0,0\n",
    "            * ...\n",
    "        * ...\n",
    "    * Student 1:\n",
    "        * ...\n",
    "\n",
    "Strip naming convention:\n",
    "\n",
    "* Eurecom_NNN_picXG_III_XXXX_YYYY.PNG -> Strip from:\n",
    "    * position (XXXX,YYYY) (top, left), from: \n",
    "    * image picXG_III, from:\n",
    "    * Student with id NNN\n",
    "\n",
    "Then, the function get_batch generates a random sample from the dataset. It allows to draw either from the training or validation split of the dataset (defined at the level of the students, 0.8 of the students are in train and the rest in validation).\n",
    "The generated batch has the following shape:\n",
    "\n",
    "* (label, (batch_size, examples, height, width, channels), (batch_size, examples, height, width, channels))  Since images are grayscale, in fact channels should be 1\n",
    "\n",
    "More clearly:\n",
    "\n",
    "* batch:\n",
    "    * label 0:\n",
    "        * strip_x-y from Image j from Student i  <---------->  strip_w-t from Image k from Student l\n",
    "        * strip_x-y from Image j2 from Student i  <---------->  strip_w-t from Image k2 from Student l\n",
    "        * strip_x-y from Image j3 from Student i  <---------->  strip_w-t from Image k3 from Student l\n",
    "        * strip_x-y from Image j4 from Student i  <---------->  strip_w-t from Image k4 from Student l\n",
    "    * label 0:\n",
    "        * ...\n",
    "        \n",
    "    * label 1:\n",
    "        * strip_x-y from Image j from Student i  <---------->  strip_w-t from Image k from Student i\n",
    "        * strip_x-y from Image j2 from Student i  <---------->  strip_w-t from Image k2 from Student i\n",
    "        * strip_x-y from Image j3 from Student i  <---------->  strip_w-t from Image k3 from Student i\n",
    "        * strip_x-y from Image j4 from Student i  <---------->  strip_w-t from Image k4 from Student i\n",
    "    * label 1:\n",
    "        * ...\n",
    "        \n",
    "Note that ${i \\neq l, j \\neq j_i}$ etc. \n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pipeline\n",
    "\n",
    "During training, a batch is generated at random from the training slice, ensuring the previously described invariant. The next steps are as follows:\n",
    "\n",
    "* Images are normalized\n",
    "* We define the left and right input, according to the tuple in the pair (for label == 0, all strips belong to the same camera and position, while for label == 1, each input belongs to a different camera.)\n",
    "* All images from each input are averaged element-wise (separetely, of course)\n",
    "* The result is fed, one at a time to a CNN model defined at sequential_block. Note that the weights are the same for each input\n",
    "* The CNN outputs a feature vector of size 2048\n",
    "* The output from each of the inputs is compared by calculating the absolute distance between the two vectors\n",
    "* This result is fed into a final Dense layer, for classification, with sigmoid activation. \n",
    "    * A result close to 0 indicates that the strips from each input belong to different devices.\n",
    "    * A result close to 1 indicates that the strips from each input where taken by the same device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random as rng\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data = []\n",
    "with open('/home/data/strips_socrates/dataset_info.json') as json_file: \n",
    "    info_data = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIP_SIZE = 256\n",
    "SUB_STRIP_SIZE = 128\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(info_data, batch_size, examples, with_id=False, dataset = 'train'):\n",
    "    pairs = [np.zeros((batch_size, examples, 128, 128, CHANNELS)) for i in range(2)]\n",
    "    labels = np.zeros((batch_size, ))\n",
    "    labels[batch_size//2:] = 1\n",
    "    split_lhs = 0\n",
    "    split_rhs = len(info_data)-1\n",
    "    if dataset == 'train':\n",
    "        split_rhs = int((len(info_data)-1)*0.55)\n",
    "    elif dataset == 'validation':\n",
    "        split_lhs = int((len(info_data)-1)*0.55)+1\n",
    "        split_rhs = int((len(info_data)-1)*0.62)\n",
    "    elif dataset == 'test':\n",
    "        split_lhs = int((len(info_data)-1)*0.62)+1\n",
    "        split_rhs = int((len(info_data)-1))\n",
    "    num_stds = split_rhs-split_lhs+1\n",
    "    students = [rng.randint(split_lhs, split_rhs) for _ in range(batch_size)]\n",
    "    strips_loc = [rng.randint(0, len(info_data[i][0][1])-1) for i in students]\n",
    "    id_pairs = []\n",
    "    for i in range(batch_size):\n",
    "        std = students[i]\n",
    "        avl_images = [j for j in range(len(info_data[std]))]\n",
    "        exl = rng.sample(avl_images, examples)\n",
    "        avl_images = [j for j in avl_images if j not in exl]\n",
    "        srl = strips_loc[i]\n",
    "        strips1 = [ info_data[std][e][1][srl] for e in exl ]\n",
    "        strips2 = []\n",
    "        if i >= batch_size // 2:\n",
    "            exl2 = rng.sample(avl_images, examples)\n",
    "            strips2 = [info_data[std][e][1][srl] for e in exl2]\n",
    "        else:\n",
    "            std2 = split_lhs + (((std-split_lhs) + rng.randint(1, num_stds-1)) % num_stds)\n",
    "            srl2 = rng.randint(0, len(info_data[std2][0][1])-1)\n",
    "            avl_images2 = [j for j in range(len(info_data[std2]))]\n",
    "            exl2 = rng.sample(avl_images2, examples)\n",
    "            strips2 = [ info_data[std2][e][1][srl2] for e in exl2]\n",
    "        for k in range(examples):\n",
    "            rch = rng.randint(0, 127)\n",
    "            rcw = rng.randint(0, 127)\n",
    "            im1 = cv2.imread(strips1[k], cv2.IMREAD_UNCHANGED)\n",
    "            if im1 is None:\n",
    "                print(f\"Error reading {strips1[k]}, dataset:{dataset}\")\n",
    "            im2 = cv2.imread(strips2[k], cv2.IMREAD_UNCHANGED)\n",
    "            if im2 is None:\n",
    "                print(f\"Error reading {strips2[k]}, dataset:{dataset}\")\n",
    "            pairs[0][i,k,...] = im1[rch:rch+128,rcw:rcw+128,:]/255\n",
    "            pairs[1][i,k,...] = im2[rch:rch+128,rcw:rcw+128,:]/255\n",
    "        id_pairs.append((strips1, strips2))\n",
    "    if with_id:\n",
    "        return pairs, labels, id_pairs\n",
    "    else:\n",
    "        return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,l,ids = get_batch(info_data, 10, 4, True, dataset=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(p[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r,s in zip(l, ids):\n",
    "    print(r)\n",
    "    for a in s:\n",
    "        for b in a:\n",
    "            print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(info_data, examples, batch_size, dataset='train'):\n",
    "    while True:\n",
    "        pairs, labels = get_batch(info_data, batch_size, examples, False, dataset=dataset)\n",
    "        yield(pairs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_block(input_shape = (128,128, 3), base_filters=64):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(base_filters, (3,3), input_shape=input_shape, activation=\"relu\"))\n",
    "    #model.add(layers.SpatialDropout2D(0.1))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*2, (3,3) ,activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (3,3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (3,3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    model.add(layers.Conv2D(base_filters*4, (3,3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "    model.add(layers.BatchNormalization(renorm=True))\n",
    "    #model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(base_filters*32, activation='relu'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = sequential_block(base_filters=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape = (4, 128,128,3), base_filters=16):\n",
    "\n",
    "    left_input = layers.Input(input_shape)\n",
    "    right_input = layers.Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    subshape = input_shape[1:4]\n",
    "    model = sequential_block(subshape, base_filters)\n",
    "    \n",
    "    #average noise patterns for image examples\n",
    "    avged_input_l = layers.Average()([left_input[:,k,...] for k in range(input_shape[0])])\n",
    "    avged_input_r = layers.Average()([right_input[:,k,...] for k in range(input_shape[0])])\n",
    "\n",
    "    encoded_l = model(avged_input_l)\n",
    "    encoded_r = model(avged_input_r)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = layers.Dense(1,activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = keras.Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                                      Output Shape                                Param #                 Connected to                                                      \n",
      "========================================================================================================================================================================================================\n",
      "input_1 (InputLayer)                                              [(None, 4, 128, 128, 3)]                    0                                                                                         \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "input_2 (InputLayer)                                              [(None, 4, 128, 128, 3)]                    0                                                                                         \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (SlicingOpLambda)                        (None, 128, 128, 3)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (SlicingOpLambda)                      (None, 128, 128, 3)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (SlicingOpLambda)                      (None, 128, 128, 3)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_3 (SlicingOpLambda)                      (None, 128, 128, 3)                         0                       input_1[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_4 (SlicingOpLambda)                      (None, 128, 128, 3)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_5 (SlicingOpLambda)                      (None, 128, 128, 3)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_6 (SlicingOpLambda)                      (None, 128, 128, 3)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_7 (SlicingOpLambda)                      (None, 128, 128, 3)                         0                       input_2[0][0]                                                     \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "average (Average)                                                 (None, 128, 128, 3)                         0                       tf.__operators__.getitem[0][0]                                    \n",
      "                                                                                                                                      tf.__operators__.getitem_1[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_2[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_3[0][0]                                  \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "average_1 (Average)                                               (None, 128, 128, 3)                         0                       tf.__operators__.getitem_4[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_5[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_6[0][0]                                  \n",
      "                                                                                                                                      tf.__operators__.getitem_7[0][0]                                  \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "sequential (Sequential)                                           (None, 1024)                                3669600                 average[0][0]                                                     \n",
      "                                                                                                                                      average_1[0][0]                                                   \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "lambda (Lambda)                                                   (None, 1024)                                0                       sequential[0][0]                                                  \n",
      "                                                                                                                                      sequential[1][0]                                                  \n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "dense_1 (Dense)                                                   (None, 1)                                   1025                    lambda[0][0]                                                      \n",
      "========================================================================================================================================================================================================\n",
      "Total params: 3,670,625\n",
      "Trainable params: 3,668,225\n",
      "Non-trainable params: 2,400\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model(base_filters=32)\n",
    "model.summary(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-5\n",
    "end_learning_rate = 1e-7\n",
    "decay_steps = 12800 * 5\n",
    "lr_schedule = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate, decay_steps, end_learning_rate, power=1\n",
    ")\n",
    "model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        metrics=[keras.metrics.Precision(name='precision'),\n",
    "             keras.metrics.Recall(name='recall'), \"accuracy\"] \n",
    "    )\n",
    "run_name = \"rand_crop05\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs/\"+run_name)\n",
    "\n",
    "checkpoint_filepath = \"checkPts/siamese_\"+run_name+\".h5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_best_only=True)\n",
    "\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 500\n",
    "examples = 4\n",
    "steps_epoch = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = get_batch(info_data, 300, 4, False, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(generate(info_data, examples, batch_size, dataset='train'), \n",
    "         epochs = epochs,\n",
    "          initial_epoch=9,\n",
    "         steps_per_epoch= steps_epoch,\n",
    "          callbacks=[tensorboard_callback,\n",
    "                     model_checkpoint_callback,\n",
    "                     early_stopping_callback],\n",
    "          validation_batch_size=16,\n",
    "          validation_data=val_set,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"last-hack02.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"checkPts/siamese_rand_crop05.h5\") #best model so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation:\n",
    "Simple, just feed pairs from the validation split and see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = get_batch(info_data, 1, 4, False, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(val_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = [y == (p>0) for y, p in zip (val_set, preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(preds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[217.  33.]\n",
      " [ 91. 159.]] 0.752 0.636\n"
     ]
    }
   ],
   "source": [
    "cs = np.zeros((2,2))\n",
    "for y,p in zip (val_set[1], preds):\n",
    "    cs[int(y), round(p[0])] += 1 \n",
    "acc = (cs[0,0] + cs [1 , 1]) / np.sum(cs)\n",
    "rec = cs[1,1] / np.sum(cs[1])\n",
    "print(cs, acc, rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended validation: \n",
    "Take two images, and compare strip by strip, then do a majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_extended(model, info_data, std1, std2, ims1, ims2, max_test_per_pair):\n",
    "    examples = len(ims1)\n",
    "    pair = [np.zeros((1, examples, 128, 128, CHANNELS)) for i in range(2)]\n",
    "    nstrips1 = len(info_data[std1][0][1])\n",
    "    nstrips2 = len(info_data[std2][0][1])\n",
    "        \n",
    "    all_strips1 = [[info_data[std1][im][1][i] for im in ims1] for i in range(nstrips1)]\n",
    "    all_strips2 = [[info_data[std2][im][1][i] for im in ims2] for i in range(nstrips2)]\n",
    "    \n",
    "    predicts = []\n",
    "    mt = [0]*max_test_per_pair\n",
    "    for strips1, strips2, _ in zip(all_strips1, all_strips2, mt):    \n",
    "        for k in range(examples):\n",
    "            s1 = strips1[k]\n",
    "            s2 = strips2[k]\n",
    "            im1 = cv2.imread(s1)[64:256-64, 64:256-64,:]\n",
    "            im2 = cv2.imread(s2)[64:256-64, 64:256-64,:]\n",
    "            if im1 is None:\n",
    "                print(f\"Error reading {strips1[k]}, dataset:{dataset}\")\n",
    "            if im2 is None:\n",
    "                print(f\"Error reading {strips2[k]}, dataset:{dataset}\")\n",
    "            pair[0][0, k,:,:,:] = im1/255\n",
    "            pair[1][0, k,:,:,:] = im2/255\n",
    "        predicts.append(model.predict(pair))\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extended(model, info_data, max_pairs, max_test_per_pair,  examples, dataset, thr):\n",
    "    split_lhs = 0\n",
    "    split_rhs = len(info_data)-1\n",
    "    if dataset == 'train':\n",
    "        split_rhs = int((len(info_data)-1)*0.52)\n",
    "    elif dataset == 'validation':\n",
    "        split_lhs = int((len(info_data)-1)*0.52)+1\n",
    "        split_rhs = int((len(info_data)-1)*0.61)\n",
    "    elif dataset == 'test':\n",
    "        split_lhs = int((len(info_data)-1)*0.61)+1\n",
    "        split_rhs = int((len(info_data)-1))\n",
    "    num_stds = split_rhs-split_lhs+1\n",
    "    students = [rng.randint(split_lhs, split_rhs) for _ in range(max_pairs)]\n",
    "    cs = np.zeros((2,2))\n",
    "    all_preds = [(0,0)]*max_pairs\n",
    "    for i in range(max_pairs):\n",
    "        std = students[i]\n",
    "        avl_images = [j for j in range(len(info_data[std]))]\n",
    "        exl = rng.sample(avl_images, examples)\n",
    "        avl_images = [j for j in avl_images if j not in exl]\n",
    "        if i >= max_pairs // 2:\n",
    "            exl2 = rng.sample(avl_images, examples)\n",
    "            preds = predict_extended(model, info_data, std, std, exl, exl2, max_test_per_pair)\n",
    "            score = sum(preds)/len(preds)\n",
    "            all_preds[i] = (1,score)\n",
    "            cs[1][int(score>thr)] += 1\n",
    "        else:\n",
    "            std2 = split_lhs + (((std-split_lhs) + rng.randint(1, num_stds-1)) % num_stds)\n",
    "            avl_images2 = [j for j in range(len(info_data[std2]))]\n",
    "            exl2 = rng.sample(avl_images2, examples)\n",
    "            preds = predict_extended(model, info_data, std, std2, exl, exl2, max_test_per_pair)\n",
    "            score = sum(preds)/len(preds)\n",
    "            all_preds[i] = (0,score)\n",
    "            cs[0][int(score>thr)] += 1\n",
    "    return all_preds, cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, cs = test_extended(model, info_data, 500, 40, 4, 'validation', 0.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [p[0] for p in preds]\n",
    "y_pred = [p[1].flatten() for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(y_true, y_pred, thr):\n",
    "    cm = np.zeros((2,2))\n",
    "    for yt, yp in zip(y_true,y_pred):\n",
    "        cm[yt][int(yp>thr)] +=1\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[223.,  27.],\n",
       "       [ 27., 223.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat(y_true,y_pred, 0.37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9535199999999999"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
